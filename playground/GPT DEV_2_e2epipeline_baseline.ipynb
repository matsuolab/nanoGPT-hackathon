{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [A Recipe for Training Neural Networks\n",
    "](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "- [Harvard CS197 AI Research Experiences](https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#heading=h.2z3yllpny6or)\n",
    "- [Unit tests for machine learning research](https://semla.polymtl.ca/wp-content/uploads/2022/11/Pablo-Unit-tests-for-ML-code-SEMLA-talk.pdf)\n",
    "- [CS 329S: Machine Learning Systems Design](https://stanford-cs329s.github.io/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the end-to-end training/evaluation skeleton + get dumb baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, 16)\n",
    "        self.linear = nn.Linear(16, vocab_size)\n",
    "        print('number of parameters:', sum(p.numel() for p in self.parameters()))\n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_index: (batch_size, sequence_length)\n",
    "        embedding = self.token_embedding_table(token_indexes)\n",
    "        logits = self.linear(embedding)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def loss_per_token(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length),\n",
    "            reduction='none'\n",
    "            )\n",
    "        # loss: (batch_size*sequence_length)\n",
    "        return loss.view(batch_size, sequence_length)\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            # logits: (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # next_token_logits: (batch_size, vocab_size)\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            # next_token_probs: (batch_size, vocab_size)\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # next_token: (batch_size, 1)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "            # token_indexes: (batch_size, sequence_length+1)\n",
    "        return token_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_int_test(cls, low, high, shape, kwargs):\n",
    "    layer = cls(**kwargs).cuda()\n",
    "    random_input = torch.randint(low, high, shape).cuda()\n",
    "    print('input shape:', random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print('output shape:', output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 8448\n",
      "input shape: torch.Size([4, 1024])\n",
      "output shape: torch.Size([4, 1024, 256])\n"
     ]
    }
   ],
   "source": [
    "test_cls = BigramLanguageModel\n",
    "batch_size = 4\n",
    "context_length = 1024\n",
    "vocab_size = 256\n",
    "\n",
    "kwargs = {'vocab_size': vocab_size}\n",
    "output = rand_int_test(test_cls, 0, vocab_size, (batch_size, context_length), kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1658481\n",
      "random guess loss: 10.82490511970208\n",
      "tensor(10.9950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 1024]) tensor(10.9950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor([[11.4311, 11.6552, 10.2010,  ..., 10.5417, 10.6344, 11.4137],\n",
      "        [11.0913, 10.3161, 10.8965,  ..., 11.6884, 11.4491, 10.4440],\n",
      "        [12.3048, 10.9655, 10.6260,  ..., 10.9756, 11.2433, 10.6060],\n",
      "        [10.5069, 10.6218, 11.0385,  ..., 11.9397, 10.6035, 10.4034]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from data import get_batch, enc\n",
    "import tiktoken\n",
    "import math\n",
    "\n",
    "x, y = get_batch(batch_size, context_length, 'train')\n",
    "vocab_size = tiktoken.get_encoding(\"gpt2\").n_vocab\n",
    "model = BigramLanguageModel(vocab_size).cuda()\n",
    "loss = model.loss(x.cuda(), y.cuda())\n",
    "print('random guess loss:', -math.log(1/vocab_size))\n",
    "print(loss)\n",
    "loss_per_token = model.loss_per_token(x.cuda(), y.cuda())\n",
    "print(loss_per_token.shape, loss_per_token.mean())\n",
    "print(loss_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [' must', '\\n', 'In', ' that']\n",
      "output [' must', '\\n', 'In', ' that', ' Calculator', ' HPV', 'Empty', ' LW', ' Seconds', ' Infinite', ' payoff', 'ste']\n",
      "Gold label [' must', '\\n', 'In', ' that', ' be', ' made', ' more', ' bitter', '.', ' Fear', ' o', \"'\", 'ers', 'h', 'ades', ' me', ':', '\\n', 'Good', ' expedition', ' be', ' my', ' friend', ',', ' and', ' comfort', '\\n', 'The', ' gracious', ' queen', ',', ' part', ' of', ' his', ' theme', ',', ' but', ' nothing', '\\n', 'Of', ' his', ' ill', '-', 'ta', \"'\", 'en', ' suspicion', '!', ' Come', ',', ' Cam', 'illo', ';', '\\n', 'I', ' will', ' respect', ' thee', ' as', ' a', ' father', ' if', '\\n', 'Th', 'ou', ' bear', \"'s\", 't', ' my', ' life', ' off', ' hence', ':', ' let', ' us', ' avoid', '.', '\\n', '\\n', 'C', 'AM', 'ILL', 'O', ':', '\\n', 'It', ' is', ' in', ' mine', ' authority', ' to', ' command', '\\n', 'The', ' keys', ' of', ' all', ' the', ' post', 'ern', 's', ':', ' please', ' your', ' high', 'ness', '\\n', 'To', ' take', ' the', ' urgent', ' hour', '.', ' Come', ',', ' sir', ',', ' away', '.', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'Take', ' the', ' boy', ' to', ' you', ':', ' he', ' so', ' troubles', ' me', ',', '\\n', \"'\", 'T', 'is', ' past', ' enduring', '.', '\\n', '\\n', 'First', ' Lady', ':', '\\n', 'Come', ',', ' my', ' gracious', ' lord', ',', '\\n', 'Sh', 'all', ' I', ' be', ' your', ' play', 'f', 'ellow', '?', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'No', ',', ' I', \"'ll\", ' none', ' of', ' you', '.', '\\n', '\\n', 'First', ' Lady', ':', '\\n', 'Why', ',', ' my', ' sweet', ' lord', '?', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'You', \"'ll\", ' kiss', ' me', ' hard', ' and', ' speak', ' to', ' me', ' as', ' if', '\\n', 'I', ' were', ' a', ' baby', ' still', '.', ' I', ' love', ' you', ' better', '.', '\\n', '\\n', 'Second', ' Lady', ':', '\\n', 'And', ' why', ' so', ',', ' my', ' lord', '?', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'Not', ' for', ' because', '\\n', 'Your', ' brow', 's', ' are', ' black', 'er', ';', ' yet', ' black', ' brow', 's', ',', ' they', ' say', ',', '\\n', 'Bec', 'ome', ' some', ' women', ' best', ',', ' so', ' that', ' there', ' be', ' not', '\\n', 'Too', ' much', ' hair', ' there', ',', ' but', ' in', ' a', ' semic', 'irc', 'le', '\\n', 'Or', ' a', ' half', '-', 'moon', ' made', ' with', ' a', ' pen', '.', '\\n', '\\n', 'Second', ' Lady', ':', '\\n', 'Who', ' taught', ' you', ' this', '?', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'I', ' learnt', ' it', ' out', ' of', ' women', \"'s\", ' faces', '.', ' Pr', 'ay', ' now', '\\n', 'What', ' colour', ' are', ' your', ' eyebrows', '?', '\\n', '\\n', 'First', ' Lady', ':', '\\n', 'Blue', ',', ' my', ' lord', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'N', 'ay', ',', ' that', \"'s\", ' a', ' mock', ':', ' I', ' have', ' seen', ' a', ' lady', \"'s\", ' nose', '\\n', 'That', ' has', ' been', ' blue', ',', ' but', ' not', ' her', ' eyebrows', '.', '\\n', '\\n', 'First', ' Lady', ':', '\\n', 'H', 'ark', ' ye', ';', '\\n', 'The', ' queen', ' your', ' mother', ' rounds', ' ap', 'ace', ':', ' we', ' shall', '\\n', 'Present', ' our', ' services', ' to', ' a', ' fine', ' new', ' prince', '\\n', 'One', ' of', ' these', ' days', ';', ' and', ' then', ' you', \"'\", 'ld', ' want', 'on', ' with', ' us', ',', '\\n', 'If', ' we', ' would', ' have', ' you', '.', '\\n', '\\n', 'Second', ' Lady', ':', '\\n', 'She', ' is', ' spread', ' of', ' late', '\\n', 'Int', 'o', ' a', ' good', 'ly', ' bulk', ':', ' good', ' time', ' encounter', ' her', '!', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'What', ' wisdom', ' stir', 's', ' amongst', ' you', '?', ' Come', ',', ' sir', ',', ' now', '\\n', 'I', ' am', ' for', ' you', ' again', ':', ' pray', ' you', ',', ' sit', ' by', ' us', ',', '\\n', 'And', ' tell', \" '\", 's', ' a', ' tale', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'M', 'erry', ' or', ' sad', ' shall', \"'t\", ' be', '?', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'As', ' merry', ' as', ' you', ' will', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'A', ' sad', ' tale', \"'s\", ' best', ' for', ' winter', ':', ' I', ' have', ' one', '\\n', 'Of', ' sprites', ' and', ' goblins', '.', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'Let', \"'s\", ' have', ' that', ',', ' good', ' sir', '.', '\\n', 'Come', ' on', ',', ' sit', ' down', ':', ' come', ' on', ',', ' and', ' do', ' your', ' best', '\\n', 'To', ' fright', ' me', ' with', ' your', ' sprites', ';', ' you', \"'re\", ' powerful', ' at', ' it', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'There', ' was', ' a', ' man', '--', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'N', 'ay', ',', ' come', ',', ' sit', ' down', ';', ' then', ' on', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'D', 'w', 'elt', ' by', ' a', ' church', 'yard', ':', ' I', ' will', ' tell', ' it', ' softly', ';', '\\n', 'Y', 'ond', ' cr', 'ickets', ' shall', ' not', ' hear', ' it', '.', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'Come', ' on', ',', ' then', ',', '\\n', 'And', ' give', \"'t\", ' me', ' in', ' mine', ' ear', '.', '\\n', '\\n', 'LE', 'ONT', 'ES', ':', '\\n', 'Was', ' he', ' met', ' there', '?', ' his', ' train', '?', ' Cam', 'illo', ' with', ' him', '?', '\\n', '\\n', 'First', ' Lord', ':', '\\n', 'Behind', ' the', ' tu', 'ft', ' of', ' p', 'ines', ' I', ' met', ' them', ';', ' never', '\\n', 'S', 'aw', ' I', ' men', ' sc', 'our', ' so', ' on', ' their', ' way', ':', ' I', ' eyed', ' them', '\\n', 'Even', ' to', ' their', ' ships', '.', '\\n', '\\n', 'LE', 'ONT', 'ES', ':', '\\n', 'How', ' bl', 'est', ' am', ' I', '\\n', 'In', ' my', ' just', ' cens', 'ure', ',', ' in', ' my', ' true', ' opinion', '!', '\\n', 'Al', 'ack', ',', ' for', ' lesser', ' knowledge', '!', ' how', ' acc', 'ursed', '\\n', 'In', ' being', ' so', ' bl', 'est', '!', ' There', ' may', ' be', ' in', ' the', ' cup', '\\n', 'A', ' spider', ' steep', \"'d\", ',', ' and', ' one', ' may', ' drink', ',', ' depart', ',', '\\n', 'And', ' yet', ' partake', ' no', ' venom', ',', ' for', ' his', ' knowledge', '\\n', 'Is', ' not', ' infected', ':', ' but', ' if', ' one', ' present', '\\n', 'The', ' abhor', 'r', \"'d\", ' ingredient', ' to', ' his', ' eye', ',', ' make', ' known', '\\n', 'How', ' he', ' hath', ' drunk', ',', ' he', ' cracks', ' his', ' gorge', ',', ' his', ' sides', ',', '\\n', 'With', ' violent', ' he', 'fts', '.', ' I', ' have', ' drunk', ',', '\\n', 'and', ' seen', ' the', ' spider', '.', '\\n', 'Cam', 'illo', ' was', ' his', ' help', ' in', ' this', ',', ' his', ' p', 'ander', ':', '\\n', 'There', ' is', ' a', ' plot', ' against', ' my', ' life', ',', ' my', ' crown', ';', '\\n', 'All', \"'s\", ' true', ' that', ' is', ' mist', 'r', 'usted', ':', ' that', ' false', ' villain', '\\n', 'Wh', 'om', ' I', ' employ', \"'d\", ' was', ' pre', '-', 'employ', \"'d\", ' by', ' him', ':', '\\n', 'He', ' has', ' discover', \"'d\", ' my', ' design', ',', ' and', ' I', '\\n', 'Rem', 'ain', ' a', ' pinch', \"'d\", ' thing', ';', ' yea', ',', ' a', ' very', ' trick', '\\n', 'For', ' them', ' to', ' play', ' at', ' will', '.', ' How', ' came', ' the', ' post', 'ern', 's', '\\n', 'So', ' easily', ' open', '?', '\\n', '\\n', 'First', ' Lord', ':', '\\n', 'By', ' his', ' great', ' authority', ';', '\\n', 'Which', ' often', ' hath', ' no', ' less', ' prevail', \"'d\", ' than', ' so', '\\n', 'On', ' your', ' command', '.', '\\n', '\\n', 'LE', 'ONT', 'ES', ':', '\\n', 'I', ' know', \"'t\", ' too', ' well', '.', '\\n', 'Give', ' me', ' the', ' boy', ':', ' I', ' am', ' glad', ' you', ' did', ' not', ' nurse']\n"
     ]
    }
   ],
   "source": [
    "input_tokens = x[0, :4].unsqueeze(0).cuda()\n",
    "max_new_token = 8\n",
    "generated_tokens = model.generate(input_tokens, max_new_token)\n",
    "print('input', [enc.decode([i.item()]) for i in input_tokens[0]])\n",
    "print('output', [enc.decode([i.item()]) for i in generated_tokens[0]])\n",
    "print('Gold label', [enc.decode([i.item()]) for i in  x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 0 loss: 11.002180099487305\n",
      "steps: 100 loss: 10.177762985229492\n",
      "steps: 200 loss: 9.035687446594238\n",
      "steps: 300 loss: 7.741647243499756\n",
      "steps: 400 loss: 6.681490898132324\n",
      "steps: 500 loss: 5.927878379821777\n",
      "steps: 600 loss: 5.586850166320801\n",
      "steps: 700 loss: 5.336899280548096\n",
      "steps: 800 loss: 5.216601848602295\n",
      "steps: 900 loss: 5.073817253112793\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "context_length = 1024\n",
    "for steps in range(1000):\n",
    "    x, y = get_batch(batch_size, context_length, 'train')\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    loss = model.loss(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print('steps:', steps, 'loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram import Ngram\n",
    "from data import text, enc\n",
    "vocab = list(range(enc.n_vocab))\n",
    "ngram = Ngram(2, vocab)\n",
    "ngram.train(enc.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
