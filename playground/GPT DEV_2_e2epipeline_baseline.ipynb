{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [A Recipe for Training Neural Networks\n",
    "](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "- [Harvard CS197 AI Research Experiences](https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#heading=h.2z3yllpny6or)\n",
    "- [Unit tests for machine learning research](https://semla.polymtl.ca/wp-content/uploads/2022/11/Pablo-Unit-tests-for-ML-code-SEMLA-talk.pdf)\n",
    "- [CS 329S: Machine Learning Systems Design](https://stanford-cs329s.github.io/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the end-to-end training/evaluation skeleton + get dumb baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # self.bigram_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, 16)\n",
    "        self.linear = nn.Linear(16, vocab_size)\n",
    "        print('number of parameters:', sum(p.numel() for p in self.parameters()))\n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_index: (batch_size, sequence_length)\n",
    "        # logits = self.bigram_table(token_indexes)\n",
    "\n",
    "        embedding = self.token_embedding_table(token_indexes)\n",
    "        logits = self.linear(embedding)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def loss_per_token(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length),\n",
    "            reduction='none'\n",
    "            )\n",
    "        # loss: (batch_size*sequence_length)\n",
    "        return loss.view(batch_size, sequence_length)\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            # logits: (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # next_token_logits: (batch_size, vocab_size)\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            # next_token_probs: (batch_size, vocab_size)\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # next_token: (batch_size, 1)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "            # token_indexes: (batch_size, sequence_length+1)\n",
    "        return token_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_int_test(cls, low, high, shape, kwargs):\n",
    "    layer = cls(**kwargs).cuda()\n",
    "    random_input = torch.randint(low, high, shape).cuda()\n",
    "    print('input shape:', random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print('output shape:', output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 8448\n",
      "input shape: torch.Size([4, 1024])\n",
      "output shape: torch.Size([4, 1024, 256])\n"
     ]
    }
   ],
   "source": [
    "test_cls = BigramLanguageModel\n",
    "batch_size = 4\n",
    "context_length = 1024\n",
    "vocab_size = 256\n",
    "\n",
    "kwargs = {'vocab_size': vocab_size}\n",
    "output = rand_int_test(test_cls, 0, vocab_size, (batch_size, context_length), kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1658481\n",
      "random guess loss: 10.82490511970208\n",
      "tensor(10.9950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 1024]) tensor(10.9950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor([[11.4311, 11.6552, 10.2010,  ..., 10.5417, 10.6344, 11.4137],\n",
      "        [11.0913, 10.3161, 10.8965,  ..., 11.6884, 11.4491, 10.4440],\n",
      "        [12.3048, 10.9655, 10.6260,  ..., 10.9756, 11.2433, 10.6060],\n",
      "        [10.5069, 10.6218, 11.0385,  ..., 11.9397, 10.6035, 10.4034]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from data import get_batch, enc\n",
    "import tiktoken\n",
    "import math\n",
    "\n",
    "x, y = get_batch(batch_size, context_length, 'train')\n",
    "vocab_size = tiktoken.get_encoding(\"gpt2\").n_vocab\n",
    "model = BigramLanguageModel(vocab_size).cuda()\n",
    "loss = model.loss(x.cuda(), y.cuda())\n",
    "print('random guess loss:', -math.log(1/vocab_size))\n",
    "print(loss)\n",
    "loss_per_token = model.loss_per_token(x.cuda(), y.cuda())\n",
    "print(loss_per_token.shape, loss_per_token.mean())\n",
    "print(loss_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [' must', '\\n', 'In', ' that']\n",
      "output [' must', '\\n', 'In', ' that', ' Calculator', ' HPV', 'Empty', ' LW', ' Seconds', ' Infinite', ' payoff', 'ste']\n",
      "Gold label [' must', '\\n', 'In', ' that', ' be', ' made', ' more', ' bitter', '.', ' Fear', ' o', \"'\", 'ers', 'h', 'ades', ' me', ':', '\\n', 'Good', ' expedition', ' be', ' my', ' friend', ',', ' and', ' comfort', '\\n', 'The', ' gracious', ' queen', ',', ' part', ' of', ' his', ' theme', ',', ' but', ' nothing', '\\n', 'Of', ' his', ' ill', '-', 'ta', \"'\", 'en', ' suspicion', '!', ' Come', ',', ' Cam', 'illo', ';', '\\n', 'I', ' will', ' respect', ' thee', ' as', ' a', ' father', ' if', '\\n', 'Th', 'ou', ' bear', \"'s\", 't', ' my', ' life', ' off', ' hence', ':', ' let', ' us', ' avoid', '.', '\\n', '\\n', 'C', 'AM', 'ILL', 'O', ':', '\\n', 'It', ' is', ' in', ' mine', ' authority', ' to', ' command', '\\n', 'The', ' keys', ' of', ' all', ' the', ' post', 'ern', 's', ':', ' please', ' your', ' high', 'ness', '\\n', 'To', ' take', ' the', ' urgent', ' hour', '.', ' Come', ',', ' sir', ',', ' away', '.', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'Take', ' the', ' boy', ' to', ' you', ':', ' he', ' so', ' troubles', ' me', ',', '\\n', \"'\", 'T', 'is', ' past', ' enduring', '.', '\\n', '\\n', 'First', ' Lady', ':', '\\n', 'Come', ',', ' my', ' gracious', ' lord', ',', '\\n', 'Sh', 'all', ' I', ' be', ' your', ' play', 'f', 'ellow', '?', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'No', ',', ' I', \"'ll\", ' none', ' of', ' you', '.', '\\n', '\\n', 'First', ' Lady', ':', '\\n', 'Why', ',', ' my', ' sweet', ' lord', '?', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'You', \"'ll\", ' kiss', ' me', ' hard', ' and', ' speak', ' to', ' me', ' as', ' if', '\\n', 'I', ' were', ' a', ' baby', ' still', '.', ' I', ' love', ' you', ' better', '.', '\\n', '\\n', 'Second', ' Lady', ':', '\\n', 'And', ' why', ' so', ',', ' my', ' lord', '?', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'Not', ' for', ' because', '\\n', 'Your', ' brow', 's', ' are', ' black', 'er', ';', ' yet', ' black', ' brow', 's', ',', ' they', ' say', ',', '\\n', 'Bec', 'ome', ' some', ' women', ' best', ',', ' so', ' that', ' there', ' be', ' not', '\\n', 'Too', ' much', ' hair', ' there', ',', ' but', ' in', ' a', ' semic', 'irc', 'le', '\\n', 'Or', ' a', ' half', '-', 'moon', ' made', ' with', ' a', ' pen', '.', '\\n', '\\n', 'Second', ' Lady', ':', '\\n', 'Who', ' taught', ' you', ' this', '?', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'I', ' learnt', ' it', ' out', ' of', ' women', \"'s\", ' faces', '.', ' Pr', 'ay', ' now', '\\n', 'What', ' colour', ' are', ' your', ' eyebrows', '?', '\\n', '\\n', 'First', ' Lady', ':', '\\n', 'Blue', ',', ' my', ' lord', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'N', 'ay', ',', ' that', \"'s\", ' a', ' mock', ':', ' I', ' have', ' seen', ' a', ' lady', \"'s\", ' nose', '\\n', 'That', ' has', ' been', ' blue', ',', ' but', ' not', ' her', ' eyebrows', '.', '\\n', '\\n', 'First', ' Lady', ':', '\\n', 'H', 'ark', ' ye', ';', '\\n', 'The', ' queen', ' your', ' mother', ' rounds', ' ap', 'ace', ':', ' we', ' shall', '\\n', 'Present', ' our', ' services', ' to', ' a', ' fine', ' new', ' prince', '\\n', 'One', ' of', ' these', ' days', ';', ' and', ' then', ' you', \"'\", 'ld', ' want', 'on', ' with', ' us', ',', '\\n', 'If', ' we', ' would', ' have', ' you', '.', '\\n', '\\n', 'Second', ' Lady', ':', '\\n', 'She', ' is', ' spread', ' of', ' late', '\\n', 'Int', 'o', ' a', ' good', 'ly', ' bulk', ':', ' good', ' time', ' encounter', ' her', '!', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'What', ' wisdom', ' stir', 's', ' amongst', ' you', '?', ' Come', ',', ' sir', ',', ' now', '\\n', 'I', ' am', ' for', ' you', ' again', ':', ' pray', ' you', ',', ' sit', ' by', ' us', ',', '\\n', 'And', ' tell', \" '\", 's', ' a', ' tale', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'M', 'erry', ' or', ' sad', ' shall', \"'t\", ' be', '?', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'As', ' merry', ' as', ' you', ' will', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'A', ' sad', ' tale', \"'s\", ' best', ' for', ' winter', ':', ' I', ' have', ' one', '\\n', 'Of', ' sprites', ' and', ' goblins', '.', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'Let', \"'s\", ' have', ' that', ',', ' good', ' sir', '.', '\\n', 'Come', ' on', ',', ' sit', ' down', ':', ' come', ' on', ',', ' and', ' do', ' your', ' best', '\\n', 'To', ' fright', ' me', ' with', ' your', ' sprites', ';', ' you', \"'re\", ' powerful', ' at', ' it', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'There', ' was', ' a', ' man', '--', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'N', 'ay', ',', ' come', ',', ' sit', ' down', ';', ' then', ' on', '.', '\\n', '\\n', 'M', 'AM', 'ILL', 'I', 'US', ':', '\\n', 'D', 'w', 'elt', ' by', ' a', ' church', 'yard', ':', ' I', ' will', ' tell', ' it', ' softly', ';', '\\n', 'Y', 'ond', ' cr', 'ickets', ' shall', ' not', ' hear', ' it', '.', '\\n', '\\n', 'HER', 'M', 'ION', 'E', ':', '\\n', 'Come', ' on', ',', ' then', ',', '\\n', 'And', ' give', \"'t\", ' me', ' in', ' mine', ' ear', '.', '\\n', '\\n', 'LE', 'ONT', 'ES', ':', '\\n', 'Was', ' he', ' met', ' there', '?', ' his', ' train', '?', ' Cam', 'illo', ' with', ' him', '?', '\\n', '\\n', 'First', ' Lord', ':', '\\n', 'Behind', ' the', ' tu', 'ft', ' of', ' p', 'ines', ' I', ' met', ' them', ';', ' never', '\\n', 'S', 'aw', ' I', ' men', ' sc', 'our', ' so', ' on', ' their', ' way', ':', ' I', ' eyed', ' them', '\\n', 'Even', ' to', ' their', ' ships', '.', '\\n', '\\n', 'LE', 'ONT', 'ES', ':', '\\n', 'How', ' bl', 'est', ' am', ' I', '\\n', 'In', ' my', ' just', ' cens', 'ure', ',', ' in', ' my', ' true', ' opinion', '!', '\\n', 'Al', 'ack', ',', ' for', ' lesser', ' knowledge', '!', ' how', ' acc', 'ursed', '\\n', 'In', ' being', ' so', ' bl', 'est', '!', ' There', ' may', ' be', ' in', ' the', ' cup', '\\n', 'A', ' spider', ' steep', \"'d\", ',', ' and', ' one', ' may', ' drink', ',', ' depart', ',', '\\n', 'And', ' yet', ' partake', ' no', ' venom', ',', ' for', ' his', ' knowledge', '\\n', 'Is', ' not', ' infected', ':', ' but', ' if', ' one', ' present', '\\n', 'The', ' abhor', 'r', \"'d\", ' ingredient', ' to', ' his', ' eye', ',', ' make', ' known', '\\n', 'How', ' he', ' hath', ' drunk', ',', ' he', ' cracks', ' his', ' gorge', ',', ' his', ' sides', ',', '\\n', 'With', ' violent', ' he', 'fts', '.', ' I', ' have', ' drunk', ',', '\\n', 'and', ' seen', ' the', ' spider', '.', '\\n', 'Cam', 'illo', ' was', ' his', ' help', ' in', ' this', ',', ' his', ' p', 'ander', ':', '\\n', 'There', ' is', ' a', ' plot', ' against', ' my', ' life', ',', ' my', ' crown', ';', '\\n', 'All', \"'s\", ' true', ' that', ' is', ' mist', 'r', 'usted', ':', ' that', ' false', ' villain', '\\n', 'Wh', 'om', ' I', ' employ', \"'d\", ' was', ' pre', '-', 'employ', \"'d\", ' by', ' him', ':', '\\n', 'He', ' has', ' discover', \"'d\", ' my', ' design', ',', ' and', ' I', '\\n', 'Rem', 'ain', ' a', ' pinch', \"'d\", ' thing', ';', ' yea', ',', ' a', ' very', ' trick', '\\n', 'For', ' them', ' to', ' play', ' at', ' will', '.', ' How', ' came', ' the', ' post', 'ern', 's', '\\n', 'So', ' easily', ' open', '?', '\\n', '\\n', 'First', ' Lord', ':', '\\n', 'By', ' his', ' great', ' authority', ';', '\\n', 'Which', ' often', ' hath', ' no', ' less', ' prevail', \"'d\", ' than', ' so', '\\n', 'On', ' your', ' command', '.', '\\n', '\\n', 'LE', 'ONT', 'ES', ':', '\\n', 'I', ' know', \"'t\", ' too', ' well', '.', '\\n', 'Give', ' me', ' the', ' boy', ':', ' I', ' am', ' glad', ' you', ' did', ' not', ' nurse']\n"
     ]
    }
   ],
   "source": [
    "input_tokens = x[0, :4].unsqueeze(0).cuda()\n",
    "max_new_token = 8\n",
    "generated_tokens = model.generate(input_tokens, max_new_token)\n",
    "print('input', [enc.decode([i.item()]) for i in input_tokens[0]])\n",
    "print('output', [enc.decode([i.item()]) for i in generated_tokens[0]])\n",
    "print('Gold label', [enc.decode([i.item()]) for i in  x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 0 loss: 11.002180099487305\n",
      "steps: 100 loss: 10.177762985229492\n",
      "steps: 200 loss: 9.035687446594238\n",
      "steps: 300 loss: 7.741647243499756\n",
      "steps: 400 loss: 6.681490898132324\n",
      "steps: 499 loss: 6.057015895843506\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "context_length = 1024\n",
    "iterations = 500\n",
    "for steps in range(iterations):\n",
    "    x, y = get_batch(batch_size, context_length, 'train')\n",
    "    # print(x[0], y[0])\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    loss = model.loss(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print('steps:', steps, 'loss:', loss.item())\n",
    "print('steps:', steps, 'loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ['I', ' have', ' a', ' motion']\n",
      "output ['I', ' have', ' a', ' motion', ' we', ' Glou', ' depletion', 'Pont', 'ndra', ' vividly', 'object', 'Red']\n",
      "Gold label ['I', ' have', ' a', ' motion', ' much', ' imports', ' your', ' good', ';', '\\n', 'Whe', 'ret', 'o', ' if', ' you', \"'ll\", ' a', ' willing', ' ear', ' incl', 'ine', ',', '\\n', 'What', \"'s\", ' mine', ' is', ' yours', ' and', ' what', ' is', ' yours', ' is', ' mine', '.', '\\n', 'So', ',', ' bring', ' us', ' to', ' our', ' palace', ';', ' where', ' we', \"'ll\", ' show', '\\n', 'What', \"'s\", ' yet', ' behind', ',', ' that', \"'s\", ' meet', ' you', ' all', ' should', ' know', '.', '\\n', '\\n', 'SL', 'Y', ':', '\\n', 'I', \"'ll\", ' p', 'hee', 'ze', ' you', ',', ' in', ' faith', '.', '\\n', '\\n', 'Host', 'ess', ':', '\\n', 'A', ' pair', ' of', ' stocks', ',', ' you', ' rogue', '!', '\\n', '\\n', 'SL', 'Y', ':', '\\n', 'Ye', ' are', ' a', ' baggage', ':', ' the', ' S', 'lys', ' are', ' no', ' rog', 'ues', ';', ' look', ' in', '\\n', 'the', ' chron', 'icles', ';', ' we', ' came', ' in', ' with', ' Richard', ' Conquer', 'or', '.', '\\n', 'Therefore', ' p', 'aucas', ' pall', 'ab', 'ris', ';', ' let', ' the', ' world', ' slide', ':', ' s', 'essa', '!', '\\n', '\\n', 'Host', 'ess', ':', '\\n', 'You', ' will', ' not', ' pay', ' for', ' the', ' glasses', ' you', ' have', ' burst', '?', '\\n', '\\n', 'SL', 'Y', ':', '\\n', 'No', ',', ' not', ' a', ' den', 'ier', '.', ' Go', ' by', ',', ' Jer', 'on', 'im', 'y', ':', ' go', ' to', ' thy', ' cold', '\\n', 'bed', ',', ' and', ' warm', ' thee', '.', '\\n', '\\n', 'Host', 'ess', ':', '\\n', 'I', ' know', ' my', ' remedy', ';', ' I', ' must', ' go', ' fetch', ' the', '\\n', 'third', '--', 'borough', '.', '\\n', '\\n', 'SL', 'Y', ':', '\\n', 'Third', ',', ' or', ' fourth', ',', ' or', ' fifth', ' borough', ',', ' I', \"'ll\", ' answer', ' him', '\\n', 'by', ' law', ':', ' I', \"'ll\", ' not', ' bud', 'ge', ' an', ' inch', ',', ' boy', ':', ' let', ' him', ' come', ',', '\\n', 'and', ' kindly', '.', '\\n', '\\n', 'Lord', ':', '\\n', 'Hun', 'ts', 'man', ',', ' I', ' charge', ' thee', ',', ' tender', ' well', ' my', ' h', 'ounds', ':', '\\n', 'Br', 'ach', ' Mer', 'rim', 'an', ',', ' the', ' poor', ' cur', ' is', ' emb', 'oss', \"'d\", ';', '\\n', 'And', ' couple', ' Cl', 'owder', ' with', ' the', ' deep', '--', 'mouth', \"'d\", ' br', 'ach', '.', '\\n', 'S', 'aw', \"'s\", 't', ' thou', ' not', ',', ' boy', ',', ' how', ' Silver', ' made', ' it', ' good', '\\n', 'At', ' the', ' hedge', '-', 'cor', 'ner', ',', ' in', ' the', ' cold', 'est', ' fault', '?', '\\n', 'I', ' would', ' not', ' lose', ' the', ' dog', ' for', ' twenty', ' pound', '.', '\\n', '\\n', 'First', ' Hunts', 'man', ':', '\\n', 'Why', ',', ' Bel', 'man', ' is', ' as', ' good', ' as', ' he', ',', ' my', ' lord', ';', '\\n', 'He', ' cried', ' upon', ' it', ' at', ' the', ' me', 'rest', ' loss', '\\n', 'And', ' twice', ' to', '-', 'day', ' pick', \"'d\", ' out', ' the', ' dull', 'est', ' scent', ':', '\\n', 'Trust', ' me', ',', ' I', ' take', ' him', ' for', ' the', ' better', ' dog', '.', '\\n', '\\n', 'Lord', ':', '\\n', 'Th', 'ou', ' art', ' a', ' fool', ':', ' if', ' Echo', ' were', ' as', ' fleet', ',', '\\n', 'I', ' would', ' esteem', ' him', ' worth', ' a', ' dozen', ' such', '.', '\\n', 'But', ' sup', ' them', ' well', ' and', ' look', ' unto', ' them', ' all', ':', '\\n', 'To', '-', 'morrow', ' I', ' intend', ' to', ' hunt', ' again', '.', '\\n', '\\n', 'First', ' Hunts', 'man', ':', '\\n', 'I', ' will', ',', ' my', ' lord', '.', '\\n', '\\n', 'Lord', ':', '\\n', 'What', \"'s\", ' here', '?', ' one', ' dead', ',', ' or', ' drunk', '?', ' See', ',', ' d', 'oth', ' he', ' breathe', '?', '\\n', '\\n', 'Second', ' Hunts', 'man', ':', '\\n', 'He', ' breat', 'hes', ',', ' my', ' lord', '.', ' Were', ' he', ' not', ' warm', \"'d\", ' with', ' ale', ',', '\\n', 'This', ' were', ' a', ' bed', ' but', ' cold', ' to', ' sleep', ' so', ' sound', 'ly', '.', '\\n', '\\n', 'Lord', ':', '\\n', 'O', ' monstrous', ' beast', '!', ' how', ' like', ' a', ' sw', 'ine', ' he', ' lies', '!', '\\n', 'G', 'rim', ' death', ',', ' how', ' foul', ' and', ' lo', 'ath', 'some', ' is', ' th', 'ine', ' image', '!', '\\n', 'S', 'irs', ',', ' I', ' will', ' practise', ' on', ' this', ' drunken', ' man', '.', '\\n', 'What', ' think', ' you', ',', ' if', ' he', ' were', ' convey', \"'d\", ' to', ' bed', ',', '\\n', 'Wra', 'pp', \"'d\", ' in', ' sweet', ' clothes', ',', ' rings', ' put', ' upon', ' his', ' fingers', ',', '\\n', 'A', ' most', ' delicious', ' banquet', ' by', ' his', ' bed', ',', '\\n', 'And', ' brave', ' attendants', ' near', ' him', ' when', ' he', ' wakes', ',', '\\n', 'Would', ' not', ' the', ' begg', 'ar', ' then', ' forget', ' himself', '?', '\\n', '\\n', 'First', ' Hunts', 'man', ':', '\\n', 'Bel', 'ieve', ' me', ',', ' lord', ',', ' I', ' think', ' he', ' cannot', ' choose', '.', '\\n', '\\n', 'Second', ' Hunts', 'man', ':', '\\n', 'It', ' would', ' seem', ' strange', ' unto', ' him', ' when', ' he', ' w', 'aked', '.', '\\n', '\\n', 'Lord', ':', '\\n', 'Even', ' as', ' a', ' flattering', ' dream', ' or', ' worthless', ' fancy', '.', '\\n', 'Then', ' take', ' him', ' up', ' and', ' manage', ' well', ' the', ' j', 'est', ':', '\\n', 'C', 'arry', ' him', ' gently', ' to', ' my', ' faire', 'st', ' chamber', '\\n', 'And', ' hang', ' it', ' round', ' with', ' all', ' my', ' want', 'on', ' pictures', ':', '\\n', 'Bal', 'm', ' his', ' foul', ' head', ' in', ' warm', ' distilled', ' waters', '\\n', 'And', ' burn', ' sweet', ' wood', ' to', ' make', ' the', ' lodging', ' sweet', ':', '\\n', 'Pro', 'c', 'ure', ' me', ' music', ' ready', ' when', ' he', ' wakes', ',', '\\n', 'To', ' make', ' a', ' d', 'ul', 'c', 'et', ' and', ' a', ' heavenly', ' sound', ';', '\\n', 'And', ' if', ' he', ' chance', ' to', ' speak', ',', ' be', ' ready', ' straight', '\\n', 'And', ' with', ' a', ' low', ' sub', 'missive', ' reverence', '\\n', 'Say', \" '\", 'What', ' is', ' it', ' your', ' honour', ' will', ' command', \"?'\", '\\n', 'Let', ' one', ' attend', ' him', ' with', ' a', ' silver', ' basin', '\\n', 'Full', ' of', ' rose', '-', 'water', ' and', ' best', 'rew', \"'d\", ' with', ' flowers', ',', '\\n', 'Another', ' bear', ' the', ' e', 'wer', ',', ' the', ' third', ' a', ' diaper', ',', '\\n', 'And', ' say', \" '\", 'Will', \"'t\", ' please', ' your', ' lords', 'hip', ' cool', ' your', ' hands', \"?'\", '\\n', 'Some', ' one', ' be', ' ready', ' with', ' a', ' costly', ' suit', '\\n', 'And', ' ask', ' him', ' what', ' apparel', ' he', ' will', ' wear', ';', '\\n', 'Another', ' tell', ' him', ' of', ' his', ' h', 'ounds', ' and', ' horse', ',', '\\n', 'And', ' that', ' his', ' lady', ' mourn', 's', ' at', ' his', ' disease', ':', '\\n', 'Pers', 'u', 'ade', ' him', ' that', ' he', ' hath', ' been', ' lun', 'atic', ';', '\\n', 'And', ' when', ' he', ' says', ' he', ' is', ',', ' say', ' that', ' he', ' dreams', ',', '\\n', 'For', ' he', ' is', ' nothing', ' but', ' a', ' mighty', ' lord', '.', '\\n', 'This', ' do', ' and', ' do', ' it', ' kindly', ',', ' gentle', ' sir', 's', ':', '\\n', 'It', ' will', ' be', ' past', 'ime', ' passing', ' excellent', ',', '\\n', 'If', ' it', ' be', ' husband', 'ed', ' with', ' modesty', '.', '\\n', '\\n', 'First', ' Hunts', 'man', ':', '\\n', 'My', ' lord', ',', ' I', ' warrant', ' you', ' we', ' will', ' play', ' our', ' part', ',', '\\n', 'As', ' he', ' shall', ' think', ' by', ' our', ' true', ' diligence', '\\n', 'He', ' is', ' no', ' less', ' than', ' what', ' we', ' say', ' he', ' is', '.', '\\n', '\\n', 'Lord', ':', '\\n', 'Take', ' him', ' up', ' gently', ' and', ' to', ' bed', ' with', ' him', ';', '\\n', 'And', ' each', ' one', ' to', ' his', ' office', ' when', ' he', ' wakes', '.', '\\n', 'Sir', 'rah', ',', ' go', ' see', ' what', ' trumpet', \" '\", 'tis', ' that', ' sounds', ':', '\\n', 'Bel', 'ike', ',', ' some', ' noble', ' gentleman', ' that', ' means', ',', '\\n', 'T', 'rave', 'lling', ' some']\n"
     ]
    }
   ],
   "source": [
    "input_tokens = x[0, :4].unsqueeze(0).cuda()\n",
    "max_new_token = 8\n",
    "generated_tokens = model.generate(input_tokens, max_new_token)\n",
    "print('input', [enc.decode([i.item()]) for i in input_tokens[0]])\n",
    "print('output', [enc.decode([i.item()]) for i in generated_tokens[0]])\n",
    "print('Gold label', [enc.decode([i.item()]) for i in  x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen tokens:  16384000\n"
     ]
    }
   ],
   "source": [
    "print('seen tokens: ', batch_size * context_length * iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8249, device='cuda:0')\n",
      "48\n",
      "tensor(10.0816, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from ngram import Ngram\n",
    "from data import text, enc\n",
    "import torch\n",
    "vocab = list(range(enc.n_vocab))\n",
    "context_lengh = 16\n",
    "ngram = Ngram(2, vocab)\n",
    "inputs = [enc.encode(text)[:context_lengh]]\n",
    "targets = torch.LongTensor([enc.encode(text)[1:context_lengh+1]]).cuda()\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)\n",
    "epochs = (batch_size * context_length * iterations) // len(enc.encode(text))\n",
    "ngram = Ngram(2, vocab)\n",
    "print(epochs)\n",
    "for epoch in range(epochs):\n",
    "    ngram.train(enc.encode(text))\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8249, device='cuda:0')\n",
      "tensor(10.5330, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ngram = Ngram(2, vocab, 1e-3)\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)\n",
    "ngram.train(enc.encode(text))\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
