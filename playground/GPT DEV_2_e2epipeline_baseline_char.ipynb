{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [A Recipe for Training Neural Networks\n",
    "](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "- [Harvard CS197 AI Research Experiences](https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#heading=h.2z3yllpny6or)\n",
    "- [Unit tests for machine learning research](https://semla.polymtl.ca/wp-content/uploads/2022/11/Pablo-Unit-tests-for-ML-code-SEMLA-talk.pdf)\n",
    "- [CS 329S: Machine Learning Systems Design](https://stanford-cs329s.github.io/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the end-to-end training/evaluation skeleton + get dumb baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.bigram_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # self.token_embedding_table = nn.Embedding(vocab_size, 16)\n",
    "        # self.linear = nn.Linear(16, vocab_size)\n",
    "        print('number of parameters:', sum(p.numel() for p in self.parameters()))\n",
    "    \n",
    "    def forward(self, token_indexes):\n",
    "        # token_index: (batch_size, sequence_length)\n",
    "        logits = self.bigram_table(token_indexes)\n",
    "\n",
    "        # embedding = self.token_embedding_table(token_indexes)\n",
    "        # logits = self.linear(embedding)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def loss_per_token(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length),\n",
    "            reduction='none'\n",
    "            )\n",
    "        # loss: (batch_size*sequence_length)\n",
    "        return loss.view(batch_size, sequence_length)\n",
    "    \n",
    "    def loss(self, token_indexes, targets):\n",
    "        logits = self(token_indexes)\n",
    "        # logits: (batch_size, sequence_length, vocab_size)\n",
    "        # targets: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length, vocab_size = logits.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(batch_size*sequence_length, vocab_size),\n",
    "            targets.view(batch_size*sequence_length)\n",
    "            )\n",
    "        # loss: scalar\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, token_indexes, max_new_tokens):\n",
    "        # token_indexes: (batch_size, sequence_length)\n",
    "        batch_size, sequence_length = token_indexes.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(token_indexes)\n",
    "            # logits: (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            # next_token_logits: (batch_size, vocab_size)\n",
    "            next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "            # next_token_probs: (batch_size, vocab_size)\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # next_token: (batch_size, 1)\n",
    "            token_indexes = torch.cat([token_indexes, next_token], dim=1)\n",
    "            # token_indexes: (batch_size, sequence_length+1)\n",
    "        return token_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "[20, 43, 50, 50, 53, 1, 51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 49, 43, 52, 53]\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'k', 'e', 'n', 'o']\n"
     ]
    }
   ],
   "source": [
    "from data_char import text, CharTokenizer\n",
    "\n",
    "tokenizer = CharTokenizer(text)\n",
    "print(tokenizer.n_vocab)\n",
    "print(tokenizer.vocab)\n",
    "print(tokenizer.encode('Hello my name is keno'))\n",
    "print(tokenizer.decode(tokenizer.encode('Hello my name is keno')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_int_test(cls, low, high, shape, kwargs):\n",
    "    layer = cls(**kwargs).cuda()\n",
    "    random_input = torch.randint(low, high, shape).cuda()\n",
    "    print('input shape:', random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print('output shape:', output.shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 65536\n",
      "input shape: torch.Size([4, 1024])\n",
      "output shape: torch.Size([4, 1024, 256])\n"
     ]
    }
   ],
   "source": [
    "test_cls = BigramLanguageModel\n",
    "batch_size = 4\n",
    "context_length = 1024\n",
    "vocab_size = 256\n",
    "\n",
    "kwargs = {'vocab_size': vocab_size}\n",
    "output = rand_int_test(test_cls, 0, vocab_size, (batch_size, context_length), kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4225\n",
      "random guess loss: 4.174387269895637\n",
      "tensor(4.7202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 1024]) tensor(4.7202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor([[4.6262, 5.8574, 5.4585,  ..., 5.5064, 5.7386, 3.9034],\n",
      "        [5.4831, 3.7372, 4.9155,  ..., 3.7848, 4.7020, 4.1312],\n",
      "        [4.7929, 3.7371, 5.3320,  ..., 4.7322, 3.9989, 4.3654],\n",
      "        [4.2844, 5.6883, 4.1599,  ..., 5.5120, 5.1844, 3.5611]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from data_char import get_batch, enc\n",
    "import math\n",
    "\n",
    "x, y = get_batch(batch_size, context_length, 'train')\n",
    "vocab_size = enc.n_vocab\n",
    "model = BigramLanguageModel(vocab_size).cuda()\n",
    "loss = model.loss(x.cuda(), y.cuda())\n",
    "print('random guess loss:', -math.log(1/vocab_size))\n",
    "print(loss)\n",
    "loss_per_token = model.loss_per_token(x.cuda(), y.cuda())\n",
    "print(loss_per_token.shape, loss_per_token.mean())\n",
    "print(loss_per_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [['y'], [' '], ['s'], ['t']]\n",
      "output [['y'], [' '], ['s'], ['t'], ['?'], ['v'], ['d'], ['x'], ['x'], ['b'], ['D'], ['y']]\n",
      "Gold label [['y'], [' '], ['s'], ['t'], ['i'], ['n'], ['g'], [' '], ['t'], ['o'], [' '], ['h'], ['u'], ['r'], ['t'], [','], ['\\n'], ['Y'], ['e'], ['t'], [' '], ['l'], ['o'], ['o'], ['k'], [' '], ['t'], ['o'], [' '], ['h'], ['a'], ['v'], ['e'], [' '], ['t'], ['h'], ['e'], ['m'], [' '], ['b'], ['u'], ['z'], ['z'], [' '], ['t'], ['o'], [' '], ['o'], ['f'], ['f'], ['e'], ['n'], ['d'], [' '], ['t'], ['h'], ['i'], ['n'], ['e'], [' '], ['e'], ['a'], ['r'], ['s'], ['.'], ['\\n'], ['F'], ['i'], ['r'], ['s'], ['t'], [' '], ['w'], ['i'], ['l'], ['l'], [' '], ['I'], [' '], ['s'], ['e'], ['e'], [' '], ['t'], ['h'], ['e'], [' '], ['c'], ['o'], ['r'], ['o'], ['n'], ['a'], ['t'], ['i'], ['o'], ['n'], [';'], ['\\n'], ['A'], ['n'], ['d'], [' '], ['t'], ['h'], ['e'], ['n'], [' '], ['t'], ['o'], [' '], ['B'], ['r'], ['i'], ['t'], ['t'], ['a'], ['n'], ['y'], [' '], ['I'], [\"'\"], ['l'], ['l'], [' '], ['c'], ['r'], ['o'], ['s'], ['s'], [' '], ['t'], ['h'], ['e'], [' '], ['s'], ['e'], ['a'], [','], ['\\n'], ['T'], ['o'], [' '], ['e'], ['f'], ['f'], ['e'], ['c'], ['t'], [' '], ['t'], ['h'], ['i'], ['s'], [' '], ['m'], ['a'], ['r'], ['r'], ['i'], ['a'], ['g'], ['e'], [','], [' '], ['s'], ['o'], [' '], ['i'], ['t'], [' '], ['p'], ['l'], ['e'], ['a'], ['s'], ['e'], [' '], ['m'], ['y'], [' '], ['l'], ['o'], ['r'], ['d'], ['.'], ['\\n'], ['\\n'], ['E'], ['D'], ['W'], ['A'], ['R'], ['D'], [':'], ['\\n'], ['E'], ['v'], ['e'], ['n'], [' '], ['a'], ['s'], [' '], ['t'], ['h'], ['o'], ['u'], [' '], ['w'], ['i'], ['l'], ['t'], [','], [' '], ['s'], ['w'], ['e'], ['e'], ['t'], [' '], ['W'], ['a'], ['r'], ['w'], ['i'], ['c'], ['k'], [','], [' '], ['l'], ['e'], ['t'], [' '], ['i'], ['t'], [' '], ['b'], ['e'], [';'], ['\\n'], ['F'], ['o'], ['r'], [' '], ['i'], ['n'], [' '], ['t'], ['h'], ['y'], [' '], ['s'], ['h'], ['o'], ['u'], ['l'], ['d'], ['e'], ['r'], [' '], ['d'], ['o'], [' '], ['I'], [' '], ['b'], ['u'], ['i'], ['l'], ['d'], [' '], ['m'], ['y'], [' '], ['s'], ['e'], ['a'], ['t'], [','], ['\\n'], ['A'], ['n'], ['d'], [' '], ['n'], ['e'], ['v'], ['e'], ['r'], [' '], ['w'], ['i'], ['l'], ['l'], [' '], ['I'], [' '], ['u'], ['n'], ['d'], ['e'], ['r'], ['t'], ['a'], ['k'], ['e'], [' '], ['t'], ['h'], ['e'], [' '], ['t'], ['h'], ['i'], ['n'], ['g'], ['\\n'], ['W'], ['h'], ['e'], ['r'], ['e'], ['i'], ['n'], [' '], ['t'], ['h'], ['y'], [' '], ['c'], ['o'], ['u'], ['n'], ['s'], ['e'], ['l'], [' '], ['a'], ['n'], ['d'], [' '], ['c'], ['o'], ['n'], ['s'], ['e'], ['n'], ['t'], [' '], ['i'], ['s'], [' '], ['w'], ['a'], ['n'], ['t'], ['i'], ['n'], ['g'], ['.'], ['\\n'], ['R'], ['i'], ['c'], ['h'], ['a'], ['r'], ['d'], [','], [' '], ['I'], [' '], ['w'], ['i'], ['l'], ['l'], [' '], ['c'], ['r'], ['e'], ['a'], ['t'], ['e'], [' '], ['t'], ['h'], ['e'], ['e'], [' '], ['D'], ['u'], ['k'], ['e'], [' '], ['o'], ['f'], [' '], ['G'], ['l'], ['o'], ['u'], ['c'], ['e'], ['s'], ['t'], ['e'], ['r'], [','], ['\\n'], ['A'], ['n'], ['d'], [' '], ['G'], ['e'], ['o'], ['r'], ['g'], ['e'], [','], [' '], ['o'], ['f'], [' '], ['C'], ['l'], ['a'], ['r'], ['e'], ['n'], ['c'], ['e'], [':'], [' '], ['W'], ['a'], ['r'], ['w'], ['i'], ['c'], ['k'], [','], [' '], ['a'], ['s'], [' '], ['o'], ['u'], ['r'], ['s'], ['e'], ['l'], ['f'], [','], ['\\n'], ['S'], ['h'], ['a'], ['l'], ['l'], [' '], ['d'], ['o'], [' '], ['a'], ['n'], ['d'], [' '], ['u'], ['n'], ['d'], ['o'], [' '], ['a'], ['s'], [' '], ['h'], ['i'], ['m'], [' '], ['p'], ['l'], ['e'], ['a'], ['s'], ['e'], ['t'], ['h'], [' '], ['b'], ['e'], ['s'], ['t'], ['.'], ['\\n'], ['\\n'], ['R'], ['I'], ['C'], ['H'], ['A'], ['R'], ['D'], [':'], ['\\n'], ['L'], ['e'], ['t'], [' '], ['m'], ['e'], [' '], ['b'], ['e'], [' '], ['D'], ['u'], ['k'], ['e'], [' '], ['o'], ['f'], [' '], ['C'], ['l'], ['a'], ['r'], ['e'], ['n'], ['c'], ['e'], [','], [' '], ['G'], ['e'], ['o'], ['r'], ['g'], ['e'], [' '], ['o'], ['f'], [' '], ['G'], ['l'], ['o'], ['u'], ['c'], ['e'], ['s'], ['t'], ['e'], ['r'], [';'], ['\\n'], ['F'], ['o'], ['r'], [' '], ['G'], ['l'], ['o'], ['u'], ['c'], ['e'], ['s'], ['t'], ['e'], ['r'], [\"'\"], ['s'], [' '], ['d'], ['u'], ['k'], ['e'], ['d'], ['o'], ['m'], [' '], ['i'], ['s'], [' '], ['t'], ['o'], ['o'], [' '], ['o'], ['m'], ['i'], ['n'], ['o'], ['u'], ['s'], ['.'], ['\\n'], ['\\n'], ['W'], ['A'], ['R'], ['W'], ['I'], ['C'], ['K'], [':'], ['\\n'], ['T'], ['u'], ['t'], [','], [' '], ['t'], ['h'], ['a'], ['t'], [\"'\"], ['s'], [' '], ['a'], [' '], ['f'], ['o'], ['o'], ['l'], ['i'], ['s'], ['h'], [' '], ['o'], ['b'], ['s'], ['e'], ['r'], ['v'], ['a'], ['t'], ['i'], ['o'], ['n'], [':'], ['\\n'], ['R'], ['i'], ['c'], ['h'], ['a'], ['r'], ['d'], [','], [' '], ['b'], ['e'], [' '], ['D'], ['u'], ['k'], ['e'], [' '], ['o'], ['f'], [' '], ['G'], ['l'], ['o'], ['u'], ['c'], ['e'], ['s'], ['t'], ['e'], ['r'], ['.'], [' '], ['N'], ['o'], ['w'], [' '], ['t'], ['o'], [' '], ['L'], ['o'], ['n'], ['d'], ['o'], ['n'], [','], ['\\n'], ['T'], ['o'], [' '], ['s'], ['e'], ['e'], [' '], ['t'], ['h'], ['e'], ['s'], ['e'], [' '], ['h'], ['o'], ['n'], ['o'], ['u'], ['r'], ['s'], [' '], ['i'], ['n'], [' '], ['p'], ['o'], ['s'], ['s'], ['e'], ['s'], ['s'], ['i'], ['o'], ['n'], ['.'], ['\\n'], ['3'], [' '], ['K'], ['I'], ['N'], ['G'], [' '], ['H'], ['E'], ['N'], ['R'], ['Y'], [' '], ['V'], ['I'], ['\\n'], ['\\n'], ['F'], ['i'], ['r'], ['s'], ['t'], [' '], ['K'], ['e'], ['e'], ['p'], ['e'], ['r'], [':'], ['\\n'], ['U'], ['n'], ['d'], ['e'], ['r'], [' '], ['t'], ['h'], ['i'], ['s'], [' '], ['t'], ['h'], ['i'], ['c'], ['k'], ['-'], ['g'], ['r'], ['o'], ['w'], ['n'], [' '], ['b'], ['r'], ['a'], ['k'], ['e'], [' '], ['w'], ['e'], [\"'\"], ['l'], ['l'], [' '], ['s'], ['h'], ['r'], ['o'], ['u'], ['d'], [' '], ['o'], ['u'], ['r'], ['s'], ['e'], ['l'], ['v'], ['e'], ['s'], [';'], ['\\n'], ['F'], ['o'], ['r'], [' '], ['t'], ['h'], ['r'], ['o'], ['u'], ['g'], ['h'], [' '], ['t'], ['h'], ['i'], ['s'], [' '], ['l'], ['a'], ['u'], ['n'], ['d'], [' '], ['a'], ['n'], ['o'], ['n'], [' '], ['t'], ['h'], ['e'], [' '], ['d'], ['e'], ['e'], ['r'], [' '], ['w'], ['i'], ['l'], ['l'], [' '], ['c'], ['o'], ['m'], ['e'], [';'], ['\\n'], ['A'], ['n'], ['d'], [' '], ['i'], ['n'], [' '], ['t'], ['h'], ['i'], ['s'], [' '], ['c'], ['o'], ['v'], ['e'], ['r'], ['t'], [' '], ['w'], ['i'], ['l'], ['l'], [' '], ['w'], ['e'], [' '], ['m'], ['a'], ['k'], ['e'], [' '], ['o'], ['u'], ['r'], [' '], ['s'], ['t'], ['a'], ['n'], ['d'], [','], ['\\n'], ['C'], ['u'], ['l'], ['l'], ['i'], ['n'], ['g'], [' '], ['t'], ['h'], ['e'], [' '], ['p'], ['r'], ['i'], ['n'], ['c'], ['i'], ['p'], ['a'], ['l'], [' '], ['o'], ['f'], [' '], ['a'], ['l'], ['l'], [' '], ['t'], ['h'], ['e'], [' '], ['d'], ['e'], ['e'], ['r'], ['.'], ['\\n'], ['\\n'], ['S'], ['e'], ['c'], ['o'], ['n'], ['d'], [' '], ['K'], ['e'], ['e'], ['p'], ['e'], ['r'], [':'], ['\\n'], ['I'], [\"'\"], ['l'], ['l'], [' '], ['s'], ['t'], ['a'], ['y'], [' '], ['a'], ['b'], ['o'], ['v'], ['e'], [' '], ['t'], ['h'], ['e'], [' '], ['h'], ['i'], ['l'], ['l'], [','], [' '], ['s'], ['o'], [' '], ['b'], ['o'], ['t'], ['h'], [' '], ['m'], ['a'], ['y'], [' '], ['s'], ['h'], ['o'], ['o'], ['t'], ['.'], ['\\n'], ['\\n'], ['F'], ['i'], ['r'], ['s'], ['t'], [' '], ['K'], ['e'], ['e'], ['p'], ['e'], ['r'], [':'], ['\\n'], ['T'], ['h'], ['a'], ['t'], [' '], ['c'], ['a'], ['n'], ['n']]\n"
     ]
    }
   ],
   "source": [
    "input_tokens = x[0, :4].unsqueeze(0).cuda()\n",
    "max_new_token = 8\n",
    "generated_tokens = model.generate(input_tokens, max_new_token)\n",
    "print('input', [enc.decode([i.item()]) for i in input_tokens[0]])\n",
    "print('output', [enc.decode([i.item()]) for i in generated_tokens[0]])\n",
    "print('Gold label', [enc.decode([i.item()]) for i in  x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 0 loss: 4.718399524688721\n",
      "steps: 100 loss: 4.5681352615356445\n",
      "steps: 200 loss: 4.431594371795654\n",
      "steps: 300 loss: 4.299322605133057\n",
      "steps: 400 loss: 4.155538558959961\n",
      "steps: 500 loss: 4.031984329223633\n",
      "steps: 600 loss: 3.921776533126831\n",
      "steps: 700 loss: 3.828066825866699\n",
      "steps: 800 loss: 3.71785569190979\n",
      "steps: 900 loss: 3.628220558166504\n",
      "steps: 999 loss: 3.539994239807129\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "context_length = 1024\n",
    "iterations = 1000\n",
    "for steps in range(iterations):\n",
    "    x, y = get_batch(batch_size, context_length, 'train')\n",
    "    # print(x[0], y[0])\n",
    "    x, y = x.cuda(), y.cuda()\n",
    "    loss = model.loss(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 100 == 0:\n",
    "        print('steps:', steps, 'loss:', loss.item())\n",
    "print('steps:', steps, 'loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [['e'], ['s'], [' '], ['i']]\n",
      "output [['e'], ['s'], [' '], ['i'], ['R'], ['Z'], [\"'\"], ['\\n'], ['d'], ['c'], [\"'\"], ['?']]\n",
      "Gold label [['e'], ['s'], [' '], ['i'], ['n'], [' '], ['a'], [' '], ['m'], ['i'], ['l'], ['e'], ['-'], ['a'], ['.'], ['\\n'], ['\\n'], ['F'], ['L'], ['O'], ['R'], ['I'], ['Z'], ['E'], ['L'], [':'], ['\\n'], ['T'], ['h'], ['e'], ['s'], ['e'], [' '], ['y'], ['o'], ['u'], ['r'], [' '], ['u'], ['n'], ['u'], ['s'], ['u'], ['a'], ['l'], [' '], ['w'], ['e'], ['e'], ['d'], ['s'], [' '], ['t'], ['o'], [' '], ['e'], ['a'], ['c'], ['h'], [' '], ['p'], ['a'], ['r'], ['t'], [' '], ['o'], ['f'], [' '], ['y'], ['o'], ['u'], ['\\n'], ['D'], ['o'], [' '], ['g'], ['i'], ['v'], ['e'], [' '], ['a'], [' '], ['l'], ['i'], ['f'], ['e'], [':'], [' '], ['n'], ['o'], [' '], ['s'], ['h'], ['e'], ['p'], ['h'], ['e'], ['r'], ['d'], ['e'], ['s'], ['s'], [','], [' '], ['b'], ['u'], ['t'], [' '], ['F'], ['l'], ['o'], ['r'], ['a'], ['\\n'], ['P'], ['e'], ['e'], ['r'], ['i'], ['n'], ['g'], [' '], ['i'], ['n'], [' '], ['A'], ['p'], ['r'], ['i'], ['l'], [\"'\"], ['s'], [' '], ['f'], ['r'], ['o'], ['n'], ['t'], ['.'], [' '], ['T'], ['h'], ['i'], ['s'], [' '], ['y'], ['o'], ['u'], ['r'], [' '], ['s'], ['h'], ['e'], ['e'], ['p'], ['-'], ['s'], ['h'], ['e'], ['a'], ['r'], ['i'], ['n'], ['g'], ['\\n'], ['I'], ['s'], [' '], ['a'], ['s'], [' '], ['a'], [' '], ['m'], ['e'], ['e'], ['t'], ['i'], ['n'], ['g'], [' '], ['o'], ['f'], [' '], ['t'], ['h'], ['e'], [' '], ['p'], ['e'], ['t'], ['t'], ['y'], [' '], ['g'], ['o'], ['d'], ['s'], [','], ['\\n'], ['A'], ['n'], ['d'], [' '], ['y'], ['o'], ['u'], [' '], ['t'], ['h'], ['e'], [' '], ['q'], ['u'], ['e'], ['e'], ['n'], [' '], ['o'], ['n'], [\"'\"], ['t'], ['.'], ['\\n'], ['\\n'], ['P'], ['E'], ['R'], ['D'], ['I'], ['T'], ['A'], [':'], ['\\n'], ['S'], ['i'], ['r'], [','], [' '], ['m'], ['y'], [' '], ['g'], ['r'], ['a'], ['c'], ['i'], ['o'], ['u'], ['s'], [' '], ['l'], ['o'], ['r'], ['d'], [','], ['\\n'], ['T'], ['o'], [' '], ['c'], ['h'], ['i'], ['d'], ['e'], [' '], ['a'], ['t'], [' '], ['y'], ['o'], ['u'], ['r'], [' '], ['e'], ['x'], ['t'], ['r'], ['e'], ['m'], ['e'], ['s'], [' '], ['i'], ['t'], [' '], ['n'], ['o'], ['t'], [' '], ['b'], ['e'], ['c'], ['o'], ['m'], ['e'], ['s'], [' '], ['m'], ['e'], [':'], ['\\n'], ['O'], [','], [' '], ['p'], ['a'], ['r'], ['d'], ['o'], ['n'], [','], [' '], ['t'], ['h'], ['a'], ['t'], [' '], ['I'], [' '], ['n'], ['a'], ['m'], ['e'], [' '], ['t'], ['h'], ['e'], ['m'], ['!'], [' '], ['Y'], ['o'], ['u'], ['r'], [' '], ['h'], ['i'], ['g'], ['h'], [' '], ['s'], ['e'], ['l'], ['f'], [','], ['\\n'], ['T'], ['h'], ['e'], [' '], ['g'], ['r'], ['a'], ['c'], ['i'], ['o'], ['u'], ['s'], [' '], ['m'], ['a'], ['r'], ['k'], [' '], ['o'], [\"'\"], [' '], ['t'], ['h'], ['e'], [' '], ['l'], ['a'], ['n'], ['d'], [','], [' '], ['y'], ['o'], ['u'], [' '], ['h'], ['a'], ['v'], ['e'], [' '], ['o'], ['b'], ['s'], ['c'], ['u'], ['r'], ['e'], ['d'], ['\\n'], ['W'], ['i'], ['t'], ['h'], [' '], ['a'], [' '], ['s'], ['w'], ['a'], ['i'], ['n'], [\"'\"], ['s'], [' '], ['w'], ['e'], ['a'], ['r'], ['i'], ['n'], ['g'], [','], [' '], ['a'], ['n'], ['d'], [' '], ['m'], ['e'], [','], [' '], ['p'], ['o'], ['o'], ['r'], [' '], ['l'], ['o'], ['w'], ['l'], ['y'], [' '], ['m'], ['a'], ['i'], ['d'], [','], ['\\n'], ['M'], ['o'], ['s'], ['t'], [' '], ['g'], ['o'], ['d'], ['d'], ['e'], ['s'], ['s'], ['-'], ['l'], ['i'], ['k'], ['e'], [' '], ['p'], ['r'], ['a'], ['n'], ['k'], [\"'\"], ['d'], [' '], ['u'], ['p'], [':'], [' '], ['b'], ['u'], ['t'], [' '], ['t'], ['h'], ['a'], ['t'], [' '], ['o'], ['u'], ['r'], [' '], ['f'], ['e'], ['a'], ['s'], ['t'], ['s'], ['\\n'], ['I'], ['n'], [' '], ['e'], ['v'], ['e'], ['r'], ['y'], [' '], ['m'], ['e'], ['s'], ['s'], [' '], ['h'], ['a'], ['v'], ['e'], [' '], ['f'], ['o'], ['l'], ['l'], ['y'], [' '], ['a'], ['n'], ['d'], [' '], ['t'], ['h'], ['e'], [' '], ['f'], ['e'], ['e'], ['d'], ['e'], ['r'], ['s'], ['\\n'], ['D'], ['i'], ['g'], ['e'], ['s'], ['t'], [' '], ['i'], ['t'], [' '], ['w'], ['i'], ['t'], ['h'], [' '], ['a'], [' '], ['c'], ['u'], ['s'], ['t'], ['o'], ['m'], [','], [' '], ['I'], [' '], ['s'], ['h'], ['o'], ['u'], ['l'], ['d'], [' '], ['b'], ['l'], ['u'], ['s'], ['h'], ['\\n'], ['T'], ['o'], [' '], ['s'], ['e'], ['e'], [' '], ['y'], ['o'], ['u'], [' '], ['s'], ['o'], [' '], ['a'], ['t'], ['t'], ['i'], ['r'], ['e'], ['d'], [','], [' '], ['s'], ['w'], ['o'], ['r'], ['n'], [','], [' '], ['I'], [' '], ['t'], ['h'], ['i'], ['n'], ['k'], [','], ['\\n'], ['T'], ['o'], [' '], ['s'], ['h'], ['o'], ['w'], [' '], ['m'], ['y'], ['s'], ['e'], ['l'], ['f'], [' '], ['a'], [' '], ['g'], ['l'], ['a'], ['s'], ['s'], ['.'], ['\\n'], ['\\n'], ['F'], ['L'], ['O'], ['R'], ['I'], ['Z'], ['E'], ['L'], [':'], ['\\n'], ['I'], [' '], ['b'], ['l'], ['e'], ['s'], ['s'], [' '], ['t'], ['h'], ['e'], [' '], ['t'], ['i'], ['m'], ['e'], ['\\n'], ['W'], ['h'], ['e'], ['n'], [' '], ['m'], ['y'], [' '], ['g'], ['o'], ['o'], ['d'], [' '], ['f'], ['a'], ['l'], ['c'], ['o'], ['n'], [' '], ['m'], ['a'], ['d'], ['e'], [' '], ['h'], ['e'], ['r'], [' '], ['f'], ['l'], ['i'], ['g'], ['h'], ['t'], [' '], ['a'], ['c'], ['r'], ['o'], ['s'], ['s'], ['\\n'], ['T'], ['h'], ['y'], [' '], ['f'], ['a'], ['t'], ['h'], ['e'], ['r'], [\"'\"], ['s'], [' '], ['g'], ['r'], ['o'], ['u'], ['n'], ['d'], ['.'], ['\\n'], ['\\n'], ['P'], ['E'], ['R'], ['D'], ['I'], ['T'], ['A'], [':'], ['\\n'], ['N'], ['o'], ['w'], [' '], ['J'], ['o'], ['v'], ['e'], [' '], ['a'], ['f'], ['f'], ['o'], ['r'], ['d'], [' '], ['y'], ['o'], ['u'], [' '], ['c'], ['a'], ['u'], ['s'], ['e'], ['!'], ['\\n'], ['T'], ['o'], [' '], ['m'], ['e'], [' '], ['t'], ['h'], ['e'], [' '], ['d'], ['i'], ['f'], ['f'], ['e'], ['r'], ['e'], ['n'], ['c'], ['e'], [' '], ['f'], ['o'], ['r'], ['g'], ['e'], ['s'], [' '], ['d'], ['r'], ['e'], ['a'], ['d'], [';'], [' '], ['y'], ['o'], ['u'], ['r'], [' '], ['g'], ['r'], ['e'], ['a'], ['t'], ['n'], ['e'], ['s'], ['s'], ['\\n'], ['H'], ['a'], ['t'], ['h'], [' '], ['n'], ['o'], ['t'], [' '], ['b'], ['e'], ['e'], ['n'], [' '], ['u'], ['s'], ['e'], ['d'], [' '], ['t'], ['o'], [' '], ['f'], ['e'], ['a'], ['r'], ['.'], [' '], ['E'], ['v'], ['e'], ['n'], [' '], ['n'], ['o'], ['w'], [' '], ['I'], [' '], ['t'], ['r'], ['e'], ['m'], ['b'], ['l'], ['e'], ['\\n'], ['T'], ['o'], [' '], ['t'], ['h'], ['i'], ['n'], ['k'], [' '], ['y'], ['o'], ['u'], ['r'], [' '], ['f'], ['a'], ['t'], ['h'], ['e'], ['r'], [','], [' '], ['b'], ['y'], [' '], ['s'], ['o'], ['m'], ['e'], [' '], ['a'], ['c'], ['c'], ['i'], ['d'], ['e'], ['n'], ['t'], [','], ['\\n'], ['S'], ['h'], ['o'], ['u'], ['l'], ['d'], [' '], ['p'], ['a'], ['s'], ['s'], [' '], ['t'], ['h'], ['i'], ['s'], [' '], ['w'], ['a'], ['y'], [' '], ['a'], ['s'], [' '], ['y'], ['o'], ['u'], [' '], ['d'], ['i'], ['d'], [':'], [' '], ['O'], [','], [' '], ['t'], ['h'], ['e'], [' '], ['F'], ['a'], ['t'], ['e'], ['s'], ['!'], ['\\n'], ['H'], ['o'], ['w'], [' '], ['w'], ['o'], ['u'], ['l'], ['d'], [' '], ['h'], ['e'], [' '], ['l'], ['o'], ['o'], ['k'], [','], [' '], ['t'], ['o'], [' '], ['s'], ['e'], ['e'], [' '], ['h'], ['i'], ['s'], [' '], ['w'], ['o'], ['r'], ['k'], [' '], ['s'], ['o'], [' '], ['n'], ['o'], ['b'], ['l'], ['e'], ['\\n'], ['V'], ['i'], ['l'], ['e'], ['l'], ['y'], [' '], ['b'], ['o'], ['u'], ['n'], ['d'], [' '], ['u'], ['p'], ['?'], [' '], ['W'], ['h'], ['a'], ['t'], [' '], ['w'], ['o'], ['u'], ['l'], ['d'], [' ']]\n"
     ]
    }
   ],
   "source": [
    "input_tokens = x[0, :4].unsqueeze(0).cuda()\n",
    "max_new_token = 8\n",
    "generated_tokens = model.generate(input_tokens, max_new_token)\n",
    "print('input', [enc.decode([i.item()]) for i in input_tokens[0]])\n",
    "print('output', [enc.decode([i.item()]) for i in generated_tokens[0]])\n",
    "print('Gold label', [enc.decode([i.item()]) for i in  x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen tokens:  32768000\n"
     ]
    }
   ],
   "source": [
    "print('seen tokens: ', batch_size * context_length * iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1744, device='cuda:0')\n",
      "29\n",
      "after 1 epoch: tensor(2.6940, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from ngram import Ngram\n",
    "from data_char import text, enc\n",
    "import torch\n",
    "vocab = list(range(enc.n_vocab))\n",
    "context_lengh = 16\n",
    "ngram = Ngram(2, vocab)\n",
    "inputs = [enc.encode(text)[:context_lengh]]\n",
    "targets = torch.LongTensor([enc.encode(text)[1:context_lengh+1]]).cuda()\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)\n",
    "epochs = (batch_size * context_length * iterations) // len(enc.encode(text))\n",
    "print(epochs)\n",
    "ngram.train(enc.encode(text))\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print('after 1 epoch:', loss)\n",
    "# for epoch in range(epochs-1):\n",
    "#     ngram.train(enc.encode(text))\n",
    "# loss = ngram.loss(inputs, targets)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5}\n",
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f'}\n",
      "tensor(4.1744, device='cuda:0')\n",
      "tensor(2.9666, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_text = 'abcdefabcdedfabcdedf'\n",
    "check_enc = CharTokenizer(train_text)\n",
    "print(check_enc.n_vocab)\n",
    "print(check_enc.encoder)\n",
    "print(check_enc.decoder)\n",
    "ngram = Ngram(2, list(range(enc.n_vocab)))\n",
    "loss = ngram.loss([check_enc.encode(train_text)[:-1]], torch.LongTensor([check_enc.encode(train_text)[1:]]).cuda())\n",
    "print(loss)\n",
    "ngram.train(check_enc.encode(train_text))\n",
    "loss = ngram.loss([check_enc.encode(train_text)[:-1]], torch.LongTensor([check_enc.encode(train_text)[1:]]).cuda())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 3, 5, 0, 1, 2, 3, 4, 3, 5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_enc.encode(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function ngram.Ngram.__init__.<locals>.<lambda>()>,\n",
       "            {'0-0': 1,\n",
       "             '0-1': 4,\n",
       "             '0-2': 1,\n",
       "             '0-3': 1,\n",
       "             '0-4': 1,\n",
       "             '0-5': 1,\n",
       "             '0-6': 1,\n",
       "             '0-7': 1,\n",
       "             '0-8': 1,\n",
       "             '0-9': 1,\n",
       "             '0-10': 1,\n",
       "             '0-11': 1,\n",
       "             '0-12': 1,\n",
       "             '0-13': 1,\n",
       "             '0-14': 1,\n",
       "             '0-15': 1,\n",
       "             '0-16': 1,\n",
       "             '0-17': 1,\n",
       "             '0-18': 1,\n",
       "             '0-19': 1,\n",
       "             '0-20': 1,\n",
       "             '0-21': 1,\n",
       "             '0-22': 1,\n",
       "             '0-23': 1,\n",
       "             '0-24': 1,\n",
       "             '0-25': 1,\n",
       "             '0-26': 1,\n",
       "             '0-27': 1,\n",
       "             '0-28': 1,\n",
       "             '0-29': 1,\n",
       "             '0-30': 1,\n",
       "             '0-31': 1,\n",
       "             '0-32': 1,\n",
       "             '0-33': 1,\n",
       "             '0-34': 1,\n",
       "             '0-35': 1,\n",
       "             '0-36': 1,\n",
       "             '0-37': 1,\n",
       "             '0-38': 1,\n",
       "             '0-39': 1,\n",
       "             '0-40': 1,\n",
       "             '0-41': 1,\n",
       "             '0-42': 1,\n",
       "             '0-43': 1,\n",
       "             '0-44': 1,\n",
       "             '0-45': 1,\n",
       "             '0-46': 1,\n",
       "             '0-47': 1,\n",
       "             '0-48': 1,\n",
       "             '0-49': 1,\n",
       "             '0-50': 1,\n",
       "             '0-51': 1,\n",
       "             '0-52': 1,\n",
       "             '0-53': 1,\n",
       "             '0-54': 1,\n",
       "             '0-55': 1,\n",
       "             '0-56': 1,\n",
       "             '0-57': 1,\n",
       "             '0-58': 1,\n",
       "             '0-59': 1,\n",
       "             '0-60': 1,\n",
       "             '0-61': 1,\n",
       "             '0-62': 1,\n",
       "             '0-63': 1,\n",
       "             '0-64': 1,\n",
       "             '1-0': 1,\n",
       "             '1-1': 1,\n",
       "             '1-2': 4,\n",
       "             '1-3': 1,\n",
       "             '1-4': 1,\n",
       "             '1-5': 1,\n",
       "             '1-6': 1,\n",
       "             '1-7': 1,\n",
       "             '1-8': 1,\n",
       "             '1-9': 1,\n",
       "             '1-10': 1,\n",
       "             '1-11': 1,\n",
       "             '1-12': 1,\n",
       "             '1-13': 1,\n",
       "             '1-14': 1,\n",
       "             '1-15': 1,\n",
       "             '1-16': 1,\n",
       "             '1-17': 1,\n",
       "             '1-18': 1,\n",
       "             '1-19': 1,\n",
       "             '1-20': 1,\n",
       "             '1-21': 1,\n",
       "             '1-22': 1,\n",
       "             '1-23': 1,\n",
       "             '1-24': 1,\n",
       "             '1-25': 1,\n",
       "             '1-26': 1,\n",
       "             '1-27': 1,\n",
       "             '1-28': 1,\n",
       "             '1-29': 1,\n",
       "             '1-30': 1,\n",
       "             '1-31': 1,\n",
       "             '1-32': 1,\n",
       "             '1-33': 1,\n",
       "             '1-34': 1,\n",
       "             '1-35': 1,\n",
       "             '1-36': 1,\n",
       "             '1-37': 1,\n",
       "             '1-38': 1,\n",
       "             '1-39': 1,\n",
       "             '1-40': 1,\n",
       "             '1-41': 1,\n",
       "             '1-42': 1,\n",
       "             '1-43': 1,\n",
       "             '1-44': 1,\n",
       "             '1-45': 1,\n",
       "             '1-46': 1,\n",
       "             '1-47': 1,\n",
       "             '1-48': 1,\n",
       "             '1-49': 1,\n",
       "             '1-50': 1,\n",
       "             '1-51': 1,\n",
       "             '1-52': 1,\n",
       "             '1-53': 1,\n",
       "             '1-54': 1,\n",
       "             '1-55': 1,\n",
       "             '1-56': 1,\n",
       "             '1-57': 1,\n",
       "             '1-58': 1,\n",
       "             '1-59': 1,\n",
       "             '1-60': 1,\n",
       "             '1-61': 1,\n",
       "             '1-62': 1,\n",
       "             '1-63': 1,\n",
       "             '1-64': 1,\n",
       "             '2-0': 1,\n",
       "             '2-1': 1,\n",
       "             '2-2': 1,\n",
       "             '2-3': 4,\n",
       "             '2-4': 1,\n",
       "             '2-5': 1,\n",
       "             '2-6': 1,\n",
       "             '2-7': 1,\n",
       "             '2-8': 1,\n",
       "             '2-9': 1,\n",
       "             '2-10': 1,\n",
       "             '2-11': 1,\n",
       "             '2-12': 1,\n",
       "             '2-13': 1,\n",
       "             '2-14': 1,\n",
       "             '2-15': 1,\n",
       "             '2-16': 1,\n",
       "             '2-17': 1,\n",
       "             '2-18': 1,\n",
       "             '2-19': 1,\n",
       "             '2-20': 1,\n",
       "             '2-21': 1,\n",
       "             '2-22': 1,\n",
       "             '2-23': 1,\n",
       "             '2-24': 1,\n",
       "             '2-25': 1,\n",
       "             '2-26': 1,\n",
       "             '2-27': 1,\n",
       "             '2-28': 1,\n",
       "             '2-29': 1,\n",
       "             '2-30': 1,\n",
       "             '2-31': 1,\n",
       "             '2-32': 1,\n",
       "             '2-33': 1,\n",
       "             '2-34': 1,\n",
       "             '2-35': 1,\n",
       "             '2-36': 1,\n",
       "             '2-37': 1,\n",
       "             '2-38': 1,\n",
       "             '2-39': 1,\n",
       "             '2-40': 1,\n",
       "             '2-41': 1,\n",
       "             '2-42': 1,\n",
       "             '2-43': 1,\n",
       "             '2-44': 1,\n",
       "             '2-45': 1,\n",
       "             '2-46': 1,\n",
       "             '2-47': 1,\n",
       "             '2-48': 1,\n",
       "             '2-49': 1,\n",
       "             '2-50': 1,\n",
       "             '2-51': 1,\n",
       "             '2-52': 1,\n",
       "             '2-53': 1,\n",
       "             '2-54': 1,\n",
       "             '2-55': 1,\n",
       "             '2-56': 1,\n",
       "             '2-57': 1,\n",
       "             '2-58': 1,\n",
       "             '2-59': 1,\n",
       "             '2-60': 1,\n",
       "             '2-61': 1,\n",
       "             '2-62': 1,\n",
       "             '2-63': 1,\n",
       "             '2-64': 1,\n",
       "             '3-0': 1,\n",
       "             '3-1': 1,\n",
       "             '3-2': 1,\n",
       "             '3-3': 1,\n",
       "             '3-4': 4,\n",
       "             '3-5': 3,\n",
       "             '3-6': 1,\n",
       "             '3-7': 1,\n",
       "             '3-8': 1,\n",
       "             '3-9': 1,\n",
       "             '3-10': 1,\n",
       "             '3-11': 1,\n",
       "             '3-12': 1,\n",
       "             '3-13': 1,\n",
       "             '3-14': 1,\n",
       "             '3-15': 1,\n",
       "             '3-16': 1,\n",
       "             '3-17': 1,\n",
       "             '3-18': 1,\n",
       "             '3-19': 1,\n",
       "             '3-20': 1,\n",
       "             '3-21': 1,\n",
       "             '3-22': 1,\n",
       "             '3-23': 1,\n",
       "             '3-24': 1,\n",
       "             '3-25': 1,\n",
       "             '3-26': 1,\n",
       "             '3-27': 1,\n",
       "             '3-28': 1,\n",
       "             '3-29': 1,\n",
       "             '3-30': 1,\n",
       "             '3-31': 1,\n",
       "             '3-32': 1,\n",
       "             '3-33': 1,\n",
       "             '3-34': 1,\n",
       "             '3-35': 1,\n",
       "             '3-36': 1,\n",
       "             '3-37': 1,\n",
       "             '3-38': 1,\n",
       "             '3-39': 1,\n",
       "             '3-40': 1,\n",
       "             '3-41': 1,\n",
       "             '3-42': 1,\n",
       "             '3-43': 1,\n",
       "             '3-44': 1,\n",
       "             '3-45': 1,\n",
       "             '3-46': 1,\n",
       "             '3-47': 1,\n",
       "             '3-48': 1,\n",
       "             '3-49': 1,\n",
       "             '3-50': 1,\n",
       "             '3-51': 1,\n",
       "             '3-52': 1,\n",
       "             '3-53': 1,\n",
       "             '3-54': 1,\n",
       "             '3-55': 1,\n",
       "             '3-56': 1,\n",
       "             '3-57': 1,\n",
       "             '3-58': 1,\n",
       "             '3-59': 1,\n",
       "             '3-60': 1,\n",
       "             '3-61': 1,\n",
       "             '3-62': 1,\n",
       "             '3-63': 1,\n",
       "             '3-64': 1,\n",
       "             '4-0': 1,\n",
       "             '4-1': 1,\n",
       "             '4-2': 1,\n",
       "             '4-3': 3,\n",
       "             '4-4': 1,\n",
       "             '4-5': 2,\n",
       "             '4-6': 1,\n",
       "             '4-7': 1,\n",
       "             '4-8': 1,\n",
       "             '4-9': 1,\n",
       "             '4-10': 1,\n",
       "             '4-11': 1,\n",
       "             '4-12': 1,\n",
       "             '4-13': 1,\n",
       "             '4-14': 1,\n",
       "             '4-15': 1,\n",
       "             '4-16': 1,\n",
       "             '4-17': 1,\n",
       "             '4-18': 1,\n",
       "             '4-19': 1,\n",
       "             '4-20': 1,\n",
       "             '4-21': 1,\n",
       "             '4-22': 1,\n",
       "             '4-23': 1,\n",
       "             '4-24': 1,\n",
       "             '4-25': 1,\n",
       "             '4-26': 1,\n",
       "             '4-27': 1,\n",
       "             '4-28': 1,\n",
       "             '4-29': 1,\n",
       "             '4-30': 1,\n",
       "             '4-31': 1,\n",
       "             '4-32': 1,\n",
       "             '4-33': 1,\n",
       "             '4-34': 1,\n",
       "             '4-35': 1,\n",
       "             '4-36': 1,\n",
       "             '4-37': 1,\n",
       "             '4-38': 1,\n",
       "             '4-39': 1,\n",
       "             '4-40': 1,\n",
       "             '4-41': 1,\n",
       "             '4-42': 1,\n",
       "             '4-43': 1,\n",
       "             '4-44': 1,\n",
       "             '4-45': 1,\n",
       "             '4-46': 1,\n",
       "             '4-47': 1,\n",
       "             '4-48': 1,\n",
       "             '4-49': 1,\n",
       "             '4-50': 1,\n",
       "             '4-51': 1,\n",
       "             '4-52': 1,\n",
       "             '4-53': 1,\n",
       "             '4-54': 1,\n",
       "             '4-55': 1,\n",
       "             '4-56': 1,\n",
       "             '4-57': 1,\n",
       "             '4-58': 1,\n",
       "             '4-59': 1,\n",
       "             '4-60': 1,\n",
       "             '4-61': 1,\n",
       "             '4-62': 1,\n",
       "             '4-63': 1,\n",
       "             '4-64': 1,\n",
       "             '5-0': 3,\n",
       "             '5-1': 1,\n",
       "             '5-2': 1,\n",
       "             '5-3': 1,\n",
       "             '5-4': 1,\n",
       "             '5-5': 1,\n",
       "             '5-6': 1,\n",
       "             '5-7': 1,\n",
       "             '5-8': 1,\n",
       "             '5-9': 1,\n",
       "             '5-10': 1,\n",
       "             '5-11': 1,\n",
       "             '5-12': 1,\n",
       "             '5-13': 1,\n",
       "             '5-14': 1,\n",
       "             '5-15': 1,\n",
       "             '5-16': 1,\n",
       "             '5-17': 1,\n",
       "             '5-18': 1,\n",
       "             '5-19': 1,\n",
       "             '5-20': 1,\n",
       "             '5-21': 1,\n",
       "             '5-22': 1,\n",
       "             '5-23': 1,\n",
       "             '5-24': 1,\n",
       "             '5-25': 1,\n",
       "             '5-26': 1,\n",
       "             '5-27': 1,\n",
       "             '5-28': 1,\n",
       "             '5-29': 1,\n",
       "             '5-30': 1,\n",
       "             '5-31': 1,\n",
       "             '5-32': 1,\n",
       "             '5-33': 1,\n",
       "             '5-34': 1,\n",
       "             '5-35': 1,\n",
       "             '5-36': 1,\n",
       "             '5-37': 1,\n",
       "             '5-38': 1,\n",
       "             '5-39': 1,\n",
       "             '5-40': 1,\n",
       "             '5-41': 1,\n",
       "             '5-42': 1,\n",
       "             '5-43': 1,\n",
       "             '5-44': 1,\n",
       "             '5-45': 1,\n",
       "             '5-46': 1,\n",
       "             '5-47': 1,\n",
       "             '5-48': 1,\n",
       "             '5-49': 1,\n",
       "             '5-50': 1,\n",
       "             '5-51': 1,\n",
       "             '5-52': 1,\n",
       "             '5-53': 1,\n",
       "             '5-54': 1,\n",
       "             '5-55': 1,\n",
       "             '5-56': 1,\n",
       "             '5-57': 1,\n",
       "             '5-58': 1,\n",
       "             '5-59': 1,\n",
       "             '5-60': 1,\n",
       "             '5-61': 1,\n",
       "             '5-62': 1,\n",
       "             '5-63': 1,\n",
       "             '5-64': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram.ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1744, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6793, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ngram = Ngram(2, vocab, 1e-3)\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)\n",
    "ngram.train(enc.encode(text))\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1744, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2322, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "ngram = Ngram(4, vocab, 1e-3)\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)\n",
    "ngram.train(enc.encode(text))\n",
    "loss = ngram.loss(inputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram.ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
