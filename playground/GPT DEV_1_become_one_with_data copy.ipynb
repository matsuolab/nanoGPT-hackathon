{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [A Recipe for Training Neural Networks\n",
    "](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "- [Harvard CS197 AI Research Experiences](https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#heading=h.2z3yllpny6or)\n",
    "- [Unit tests for machine learning research](https://semla.polymtl.ca/wp-content/uploads/2022/11/Pablo-Unit-tests-for-ML-code-SEMLA-talk.pdf)\n",
    "- [CS 329S: Machine Learning Systems Design](https://stanford-cs329s.github.io/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Become one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "print(text[:100])\n",
    "train_data = text[:int(len(text)*0.9)]\n",
    "val_data = text[int(len(text)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', ' ', 'Citizen:', '\\n', 'Before', ' ', 'we', ' ', 'proceed', ' ', 'any', ' ', 'further,', ' ', 'hear', ' ', 'me', ' ', 'speak.']\n",
      "[(' ', 169892), ('\\n', 40000), ('', 7242), ('the', 5437), ('I', 4403)]\n",
      "[('open;', 1), ('standing,', 1), ('moving,', 1), ('sleep--die,', 1), (\"wink'st\", 1)]\n",
      "splitted 419785 unique_word 25673\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_string(input_string):\n",
    "    # 正規表現で改行(\\n)やスペース( )で区切り、それらも結果に含める\n",
    "    split_list = re.split(r'(\\s)', input_string)\n",
    "    return split_list\n",
    "\n",
    "first_period_index = text.index('.')\n",
    "print(split_string(text[:first_period_index+1]))\n",
    "unique_words = list(set(split_string(text)))\n",
    "\n",
    "word_count_dict = {}\n",
    "for word in split_string(text):\n",
    "    if word in word_count_dict:\n",
    "        word_count_dict[word] += 1\n",
    "    else:\n",
    "        word_count_dict[word] = 1\n",
    "# 多い順に並べ替え\n",
    "word_count_dict = dict(sorted(word_count_dict.items(), key=lambda x: -x[1]))\n",
    "# 上位・下位5件を表示\n",
    "print(list(word_count_dict.items())[:5])\n",
    "print(list(word_count_dict.items())[-5:])\n",
    "print('splitted', len(split_string(text)), 'unique_word', len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13]\n",
      "['First', ' Citizen', ':', '\\n', 'Before', ' we', ' proceed', ' any', ' further', ',', ' hear', ' me', ' speak', '.']\n",
      "[(198, {'count': 39996, 'token_id': '\\n'}), (11, {'count': 19777, 'token_id': ','}), (25, {'count': 10291, 'token_id': ':'}), (13, {'count': 7811, 'token_id': '.'}), (262, {'count': 5370, 'token_id': ' the'})]\n",
      "[(16558, {'count': 1, 'token_id': ' sphere'}), (31960, {'count': 1, 'token_id': ' Wond'}), (22194, {'count': 1, 'token_id': ' possesses'}), (29708, {'count': 1, 'token_id': ' eyel'}), (30757, {'count': 1, 'token_id': 'stroke'})]\n",
      "splitted 338025 unique_token 11706 vocab_size 50257\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_ids = enc.encode(text[:first_period_index+1])\n",
    "decoded_text = [enc.decode([encoded_id]) for encoded_id in encoded_ids]\n",
    "print(encoded_ids)\n",
    "print(decoded_text)\n",
    "\n",
    "\n",
    "unique_tokens = list(set(enc.encode(text)))\n",
    "\n",
    "token_count_dict = {}\n",
    "for token in enc.encode(text):\n",
    "    if token in token_count_dict:\n",
    "        token_count_dict[token]['count'] += 1\n",
    "    else:\n",
    "        token_count_dict[token] = {'count': 1, 'token_id': enc.decode([token])}\n",
    "# 多い順に並べ替え\n",
    "token_count_dict = dict(sorted(token_count_dict.items(), key=lambda x: -x[1]['count']))\n",
    "# 上位・下位5件を表示\n",
    "print(list(token_count_dict.items())[:5])\n",
    "print(list(token_count_dict.items())[-5:])\n",
    "print('splitted', len(enc.encode(text)), 'unique_token', len(unique_tokens), 'vocab_size', enc.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params of unigram 50257\n",
      "2525766049 126937424324593\n"
     ]
    }
   ],
   "source": [
    "from ngram import Ngram\n",
    "vocab = list(range(enc.n_vocab))\n",
    "unigram = Ngram(1, vocab)\n",
    "tokens = enc.encode(text)\n",
    "unigram.train(tokens)\n",
    "print('params of unigram', len(unigram.ngram)) \n",
    "\n",
    "\n",
    "print(enc.n_vocab ** 2, enc.n_vocab ** 3)\n",
    "# bigram = Ngram(2, vocab)\n",
    "# bigram.train(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', 39997), (',', 19778), (':', 10292), ('.', 7812), (' the', 5371)]\n",
      "[('ominated', 1), (' regress', 1), (' Collider', 1), (' informants', 1), ('<|endoftext|>', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 上位・下位5件を表示\n",
    "unigram_info = unigram.ngram\n",
    "unigram_info = dict(sorted(unigram_info.items(), key=lambda x: -x[1]))\n",
    "top_unigram = list(unigram_info.items())[:5]\n",
    "bottom_unigram = list(unigram_info.items())[-5:]\n",
    "print([(enc.decode([token[0]]), count) for token, count in top_unigram])\n",
    "print([(enc.decode([token[0]]), count) for token, count in bottom_unigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "torch.Size([4, 8])\n",
      "tensor([[  198, 30313,   262, 22397,   282,   290,   884,  3790],\n",
      "        [ 4151,   438,   198, 10418,   329,   511, 11989,    11],\n",
      "        [ 3355,   322, 12105,   287,  3426,  6729,   198,  3886],\n",
      "        [  290, 15581,  8636,    13,   198,   198, 35510,  4221]])\n",
      "[['\\n', 'Except', ' the', ' marsh', 'al', ' and', ' such', ' officers'], [' eye', '--', '\\n', 'Men', ' for', ' their', ' sons', ','], [' wall', 'ow', ' naked', ' in', ' December', ' snow', '\\n', 'By'], [' and', ' noble', ' estimate', '.', '\\n', '\\n', 'NOR', 'TH']]\n",
      "target\n",
      "torch.Size([4, 8])\n",
      "tensor([[30313,   262, 22397,   282,   290,   884,  3790,   198],\n",
      "        [  438,   198, 10418,   329,   511, 11989,    11, 17743],\n",
      "        [  322, 12105,   287,  3426,  6729,   198,  3886,  3612],\n",
      "        [15581,  8636,    13,   198,   198, 35510,  4221,  5883]])\n",
      "[['Except', ' the', ' marsh', 'al', ' and', ' such', ' officers', '\\n'], ['--', '\\n', 'Men', ' for', ' their', ' sons', ',', ' wives'], ['ow', ' naked', ' in', ' December', ' snow', '\\n', 'By', ' thinking'], [' noble', ' estimate', '.', '\\n', '\\n', 'NOR', 'TH', 'UM']]\n",
      "input:  ['\\n'] target:  'Except'\n",
      "input:  ['\\n', 'Except'] target:  ' the'\n",
      "input:  ['\\n', 'Except', ' the'] target:  ' marsh'\n",
      "input:  ['\\n', 'Except', ' the', ' marsh'] target:  'al'\n",
      "input:  ['\\n', 'Except', ' the', ' marsh', 'al'] target:  ' and'\n",
      "input:  ['\\n', 'Except', ' the', ' marsh', 'al', ' and'] target:  ' such'\n",
      "input:  ['\\n', 'Except', ' the', ' marsh', 'al', ' and', ' such'] target:  ' officers'\n",
      "input:  ['\\n', 'Except', ' the', ' marsh', 'al', ' and', ' such', ' officers'] target:  '\\n'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "seed = 1337\n",
    "torch.manual_seed(seed) \n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    index = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in index])\n",
    "    y = torch.stack([data[i+1:i+1+context_length] for i in index])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('input')\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print([[enc.decode([token])for token in sequence] for sequence in x])\n",
    "print('target')\n",
    "print(y.shape)\n",
    "print(y)\n",
    "print([[enc.decode([token])for token in sequence] for sequence in y])\n",
    "\n",
    "for t in range(context_length):\n",
    "    context = x[0, :t+1]\n",
    "    target = y[0, t]\n",
    "    print('input: ', [enc.decode([token]) for token in context], 'target: ', repr(enc.decode([target])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
