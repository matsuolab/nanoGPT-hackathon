{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [A Recipe for Training Neural Networks\n",
    "](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "- [Harvard CS197 AI Research Experiences](https://docs.google.com/document/d/1uvAbEhbgS_M-uDMTzmOWRlYxqCkogKRXdbKYYT98ooc/edit#heading=h.2z3yllpny6or)\n",
    "- [Unit tests for machine learning research](https://semla.polymtl.ca/wp-content/uploads/2022/11/Pablo-Unit-tests-for-ML-code-SEMLA-talk.pdf)\n",
    "- [CS 329S: Machine Learning Systems Design](https://stanford-cs329s.github.io/syllabus.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Become one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "print(text[:100])\n",
    "train_data = text[:int(len(text)*0.9)]\n",
    "val_data = text[int(len(text)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', ' ', 'Citizen:', '\\n', 'Before', ' ', 'we', ' ', 'proceed', ' ', 'any', ' ', 'further,', ' ', 'hear', ' ', 'me', ' ', 'speak.']\n",
      "[(' ', 169892), ('\\n', 40000), ('', 7242), ('the', 5437), ('I', 4403)]\n",
      "[('open;', 1), ('standing,', 1), ('moving,', 1), ('sleep--die,', 1), (\"wink'st\", 1)]\n",
      "splitted 419785 unique_word 25673\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_string(input_string):\n",
    "    # 正規表現で改行(\\n)やスペース( )で区切り、それらも結果に含める\n",
    "    split_list = re.split(r'(\\s)', input_string)\n",
    "    return split_list\n",
    "\n",
    "first_period_index = text.index('.')\n",
    "print(split_string(text[:first_period_index+1]))\n",
    "unique_words = list(set(split_string(text)))\n",
    "\n",
    "word_count_dict = {}\n",
    "for word in split_string(text):\n",
    "    if word in word_count_dict:\n",
    "        word_count_dict[word] += 1\n",
    "    else:\n",
    "        word_count_dict[word] = 1\n",
    "# 多い順に並べ替え\n",
    "word_count_dict = dict(sorted(word_count_dict.items(), key=lambda x: -x[1]))\n",
    "# 上位・下位5件を表示\n",
    "print(list(word_count_dict.items())[:5])\n",
    "print(list(word_count_dict.items())[-5:])\n",
    "print('splitted', len(split_string(text)), 'unique_word', len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13]\n",
      "['First', ' Citizen', ':', '\\n', 'Before', ' we', ' proceed', ' any', ' further', ',', ' hear', ' me', ' speak', '.']\n",
      "[(198, {'count': 39996, 'token_id': '\\n'}), (11, {'count': 19777, 'token_id': ','}), (25, {'count': 10291, 'token_id': ':'}), (13, {'count': 7811, 'token_id': '.'}), (262, {'count': 5370, 'token_id': ' the'})]\n",
      "[(16558, {'count': 1, 'token_id': ' sphere'}), (31960, {'count': 1, 'token_id': ' Wond'}), (22194, {'count': 1, 'token_id': ' possesses'}), (29708, {'count': 1, 'token_id': ' eyel'}), (30757, {'count': 1, 'token_id': 'stroke'})]\n",
      "splitted 338025 unique_token 11706 vocab_size 50257\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_ids = enc.encode(text[:first_period_index+1])\n",
    "decoded_text = [enc.decode([encoded_id]) for encoded_id in encoded_ids]\n",
    "print(encoded_ids)\n",
    "print(decoded_text)\n",
    "\n",
    "\n",
    "unique_tokens = list(set(enc.encode(text)))\n",
    "\n",
    "token_count_dict = {}\n",
    "for token in enc.encode(text):\n",
    "    if token in token_count_dict:\n",
    "        token_count_dict[token]['count'] += 1\n",
    "    else:\n",
    "        token_count_dict[token] = {'count': 1, 'token_id': enc.decode([token])}\n",
    "# 多い順に並べ替え\n",
    "token_count_dict = dict(sorted(token_count_dict.items(), key=lambda x: -x[1]['count']))\n",
    "# 上位・下位5件を表示\n",
    "print(list(token_count_dict.items())[:5])\n",
    "print(list(token_count_dict.items())[-5:])\n",
    "print('splitted', len(enc.encode(text)), 'unique_token', len(unique_tokens), 'vocab_size', enc.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params of unigram 50257\n",
      "2525766049 126937424324593\n"
     ]
    }
   ],
   "source": [
    "from ngram import Ngram\n",
    "vocab = list(range(enc.n_vocab))\n",
    "unigram = Ngram(1, vocab)\n",
    "tokens = enc.encode(text)\n",
    "unigram.train(tokens)\n",
    "print('params of unigram', len(unigram.ngram)) \n",
    "\n",
    "\n",
    "print(enc.n_vocab ** 2, enc.n_vocab ** 3)\n",
    "# bigram = Ngram(2, vocab)\n",
    "# bigram.train(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', 39997), (',', 19778), (':', 10292), ('.', 7812), (' the', 5371)]\n",
      "[('ominated', 1), (' regress', 1), (' Collider', 1), (' informants', 1), ('<|endoftext|>', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 上位・下位5件を表示\n",
    "unigram_info = unigram.ngram\n",
    "unigram_info = dict(sorted(unigram_info.items(), key=lambda x: -x[1]))\n",
    "top_unigram = list(unigram_info.items())[:5]\n",
    "bottom_unigram = list(unigram_info.items())[-5:]\n",
    "print([(enc.decode([token[0]]), count) for token, count in top_unigram])\n",
    "print([(enc.decode([token[0]]), count) for token, count in bottom_unigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the end-to-end training/evaluation skeleton + get dumb baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
